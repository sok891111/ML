# -*- coding: utf-8 -*-
"""타이타닉_autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ZQ681bNvX5sBFpyN11fcM5f15bfUEER
"""

!pip install kaggle
from google.colab import files
files.upload()

ls -1ha kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
# Permission Warning 이 일어나지 않도록 
!chmod 600 ~/.kaggle/kaggle.json
# 본인이 참가한 모든 대회 보기 
!kaggle competitions list

!kaggle competitions download -c titanic

!ls

batch_size = 32

import pandas as pd
import numpy as np

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

test.describe()

train.describe()

cat_dict={}
def categorical_encoding(data):
  col = data.name
  if col not in cat_dict:
    cat_dict[col] = {}
  result = []
  for x in data:
    if x not in cat_dict[col] :
      if len(cat_dict[col]) == 0:
        label = 0    
      else:
        label = max(cat_dict[col].values())+1
      cat_dict[col][x] = label
    result.append(cat_dict[col][x])
  return np.array(result)

def preprocessing_data(dataset , onehot=False):
  #name에서 title 추출
  dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.')

  #missing value 처리
  #Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'] => 컬럼은 총 12개
  #print(train.isnull().sum())
  # Age , Cabin, Embarked 3개 컬럼 missing
  # Missing value는 임의의 다른 값으로 fill 처리
  dataset['Age'].fillna(0 , inplace=True) #Age 는 0으로
  dataset['Cabin'].fillna('N00' , inplace=True) #Cabin 은 N00
  dataset['Embarked'].fillna('N' , inplace=True) #Embarked는 N으로

  #categorical data 처리
  # Sex, Ticket, Cabin, Embarked, Title
  cate_col = ['Sex' , 'Ticket' , 'Cabin' , 'Embarked' , 'Title']
  
  for col in cate_col :
    # col_label = train[col].astype('category').cat.codes
    col_label = categorical_encoding(dataset[col])
    if onehot is True:
      num = np.unique(col_label, axis=0).shape[0]
      encoding = np.eye(num)[col_label]
      dataset[col+'_encoding'] = encoding
    else:
      dataset[col+'_encoding'] = col_label

  return dataset

train = preprocessing_data(train)
train.isnull().sum() #missing value 없음
train.head()

#실제 학습에 사용할 데이터는 sequential 데이터와 labeling 된 데이터
trainset = train[['PassengerId' , 'Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding' , 'Embarked_encoding' , 'Title_encoding']]
trainset

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import math
from torch.utils.data import Dataset, DataLoader

device = torch.device('cuda:0')

test

trainset = train[['Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding' , 'Embarked_encoding' , 'Title_encoding']] #to_do : train column add
train_dataloader = DataLoader(dataset=trainset.to_numpy() , batch_size=batch_size, shuffle=False)

#custom dataset
class CustomDataset(Dataset):
  # def __init__(self, data, column):
  def __init__(self, data):
    p_data = preprocessing_data(data)
    self.data = p_data[['Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding', 'Embarked_encoding' , 'Title_encoding']] #to_do : train column add 
  
  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    return self.data.iloc[idx, 0 ] , torch.tensor(self.data.iloc[idx, 1:].to_numpy(),dtype=torch.float, device = device)

test_dataset = CustomDataset(test)
test_dataloader = DataLoader(dataset=test_dataset , batch_size = batch_size , shuffle=False)

def _same_pad(k=1, dil=1):
    # assumes stride length of 1
    # p = math.ceil((l - 1) * s - l + dil*(k - 1) + 1)
    p = math.ceil(dil*(k - 1))
    #print("padding:", p)
    return p

class ResBlock(nn.Module):
    """
        Note To Self:  using padding to "mask" the convolution is equivalent to
        either centering the convolution (no mask) or skewing the convolution to
        the left (mask).  Either way, we should end up with n timesteps.
        Also note that "masked convolution" and "casual convolution" are two
        names for the same thing.
    Args:
        d (int): size of inner track of network.
        r (int): size of dilation
        k (int): size of kernel in dilated convolution (odd numbers only)
        casual (bool): determines how to pad the casual conv layer. See notes.
    """
    def __init__(self, d, r=1, k=3, casual=False, use_bias=False):
        super(ResBlock, self).__init__()
        self.d = d # input features
        self.r = r # dilation size
        self.k = k # "masked kernel size"
        ub = use_bias
        self.layernorm1 = nn.InstanceNorm1d(num_features=2*d, affine=True) # same as LayerNorm
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1x1_1 = nn.Conv1d(2*d, d, kernel_size=1, bias=ub) # output is "d"
        self.layernorm2 = nn.InstanceNorm1d(num_features=d, affine=True)
        self.relu2 = nn.ReLU(inplace=True)
        if casual:
            padding = (_same_pad(k,r), 0)
        else:
            p = _same_pad(k,r)
            if p % 2 == 1:
                padding = [p // 2 + 1, p // 2]
            else:
                padding = (p // 2, p // 2)
        self.pad = nn.ConstantPad1d(padding, 0.)
        #self.pad = nn.ReflectionPad1d(padding) # this might be better for audio
        self.maskedconv1xk = nn.Conv1d(d, d, kernel_size=k, dilation=r, bias=ub)
        self.layernorm3 = nn.InstanceNorm1d(num_features=d, affine=True)
        self.relu3 = nn.ReLU(inplace=True)
        self.conv1x1_2 = nn.Conv1d(d, 2*d, kernel_size=1, bias=ub) # output is "2*d"

    def forward(self, input):
        x = input
        x = self.layernorm1(x)
        x = self.relu1(x)
        x = self.conv1x1_1(x)

        x = self.layernorm2(x)
        x = self.relu2(x)
        x = self.pad(x)
        x = self.maskedconv1xk(x)

        x = self.layernorm3(x)
        x = self.relu3(x)
        x = self.conv1x1_2(x)
        #print("ResBlock:", x.size(), input.size())
        x += input # add back in residual
        return x

class ResBlockSet(nn.Module):
    """
        The Bytenet encoder and decoder are made up of sets of residual blocks
        with dilations of increasing size.  These sets are then stacked upon each
        other to create the full network.
    """
    def __init__(self, d, max_r=16, k=3, casual=False , mt=None):
        super(ResBlockSet, self).__init__()
        self.d = d
        self.max_r = max_r
        self.k = k
        self.mt = mt
        rlist = [1 << x for x in range(15) if (1 << x) <= max_r]
        self.blocks = nn.Sequential(*[ResBlock(d, r, k, casual) for r in rlist])

        # if self.mt =='encoder':
        #   self.avg_pooling = nn.AvgPool1d(3, stride=2)
        # elif self.mt =='decoder':
        #   self.deconv = nn.ConvTranspose1d(d*2 , d*2 , 7, 6)

    def forward(self, input):
        x = input
        x = self.blocks(x)

        
        # if self.mt =='encoder':
        #   x = self.avg_pooling(x)
        
        # elif self.mt =='decoder':
        #   x = self.deconv(x)
     
        return x

class BytenetEncoder(nn.Module):
    """
        d = hidden units
        max_r = maximum dilation rate (paper default: 16)
        k = masked kernel size (paper default: 3)
        num_sets = number of residual sets (paper default: 6. 5x6 = 30 ResBlocks)
        a = relative length of output sequence
        b = output sequence length intercept
    """
    def __init__(self, d=800, max_r=16, k=3, num_sets=6):
        super(BytenetEncoder, self).__init__()
        self.d = d
        self.max_r = max_r
        self.k = k
        self.num_sets = num_sets
        self.pad_in = nn.ConstantPad1d((0, 1), 0.)
        self.conv_in = nn.Conv1d(d, 2*d , 1)
        self.sets = nn.Sequential()
        for i in range(num_sets):
            self.sets.add_module("set_{}".format(i+1), ResBlockSet(d, max_r, k, mt="encoder"))
        # self.conv_out = nn.Conv1d(2*d, 2*d, 1)
        self.conv_out = nn.Conv1d(2*d , 1, 1)

    def forward(self, input):
        x = input
        
        x_len = x.size(-1)
        x = self.conv_in(x)
        x = self.sets(x)
        x = self.conv_out(x)
        x = F.relu(x)
        return x

class SimpleEmbEncoder(nn.Module):
    """
        The decoder only training does not specify how to turn an integer input
        into a 2*d dimensional vector that the decoder network expects, so I
        decided to use a simple embedding network.
    """
    def __init__(self, ne, ed):
        super(SimpleEmbEncoder, self).__init__()
        self.emb = nn.Embedding(num_embeddings=ne, embedding_dim=ed)
        self.ne = ne
    def forward(self, input):
        x = torch.clamp(input, 0, self.ne)
        x = self.emb(x)
        x = x.transpose(1, 2)
        return x

class BytenetDecoder(nn.Module):
    """
        d = hidden units
        max_r = maximum dilation rate (paper default: 16)
        k = masked kernel size (paper default: 3)
        num_sets = number of residual sets (paper default: 6. 5x6 = 30 ResBlocks)
        num_classes = number of output classes (Hunter prize default: 205)
    """
    def __init__(self, d=512, max_r=16, k=3, num_sets=6, num_classes=205,
                 reduce_out=None, use_logsm=True):
        super(BytenetDecoder, self).__init__()
        self.max_r = max_r
        self.k = k
        self.d = d
        self.num_sets = num_sets
        self.use_logsm = use_logsm # this is for NLLLoss
        self.conv_in = nn.Conv1d(1,2*d,1)
        self.sets = nn.Sequential()
        for i in range(num_sets):
            self.sets.add_module("set_{}".format(i+1), ResBlockSet(d, max_r, k, True, mt="decoder"))
            if reduce_out is not None:
                r = reduce_out[i]
                if r != 0:
                    reduce_conv = nn.Conv1d(2*d, 2*d, r, r)
                    reduce_pad = nn.ConstantPad1d((0, r), 0.)
                    self.sets.add_module("reduce_pad_{}".format(i+1), reduce_pad)
                    self.sets.add_module("reduce_{}".format(i+1), reduce_conv)
        self.conv1 = nn.Conv1d(2*d, 2*d, 1)
        self.conv2 = nn.Conv1d(2*d, num_classes, 1)
        #self.logsm = nn.LogSoftmax(dim=1)
        self.fc = nn.Linear(2851, 3401)

    def forward(self, input):
        x = input
        x = self.conv_in(x)
        x = self.sets(x)
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        
        # if self.use_logsm:
            # x = self.logsm(x)
        # print(x.shape)
        # x = self.fc(x)
        return x

#@title 기본 제목 텍스트
#hyper para 설정
learning_rate = 0.00001
input_features = trainset.columns.shape[0] -1
max_r = 16
k = 3
num_sets = 6
num_classes = 205
epochs = 1
max_length=3401
feature_num = trainset.columns.shape[0] #9

next(iter(train_dataloader)).shape

encoder = BytenetEncoder(feature_num, max_r, k, num_sets=4).to(device)
decoder = BytenetDecoder(feature_num , max_r, k, num_sets=4, num_classes=feature_num, use_logsm=False).to(device)
params = [{"params": encoder.parameters()}, {"params": decoder.parameters()}]
# criterion = nn.CrossEntropyLoss()
# criterion = torch.nn.Smooth1Loss()
criterion = torch.nn.L1Loss()
optimizer = torch.optim.Adam(params)

dataset_sizes = len(train)

def train_model(encoder, decoder , criterion, optimizer , scheduler = None , num_epochs=25):
  train_loss_history = []
  for epoch in range(num_epochs):

    encoder.train()
    decoder.train()
    avg_loss = 0.0

    for idx , x in enumerate(train_dataloader):

      encoder.zero_grad()
      decoder.zero_grad()
      optimizer.zero_grad()
      
      x = x.unsqueeze(2).float().to(device)
      output = encoder(x)
      output = decoder(output)

      loss = criterion(output, x)

      loss.backward()
      optimizer.step()

      avg_loss += loss.item()

      print("Epoch {}..... Step: {}/{}........ Average Loss {}".format(epoch,(idx+1)*batch_size , dataset_sizes, avg_loss/(idx+1) ))

      train_loss_history.append(avg_loss/(idx+1)*1000)
  return encoder, decoder, train_loss_history

_, _ , train_loss_history = train_model(encoder, decoder, criterion, optimizer,None, 5000)

encoder(x).squeeze(1).cpu().detach().numpy().shape

tmp_result = []
passenger_id = []
encoder.eval()
for idx, x in enumerate(train_dataloader):
  # print(x.shape)
  x = x.unsqueeze(2).float().to(device)
  new_x = encoder(x).squeeze(1).cpu().detach().numpy()
  tmp_result.append(new_x)
  # tmp_result.append(encoder(x.unsqueeze(-1).to(device)).squeeze(1).detach().cpu().numpy())
  # passenger_id.append(id)
tmp_result = np.concatenate(tmp_result, axis=0)

#custom dataset
class SurviveDataset(Dataset):
  # def __init__(self, data, column):
  def __init__(self, data):
    p_data = preprocessing_data(data)
    self.data = p_data[['PassengerId' , 'Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding', 'Embarked_encoding' , 'Title_encoding']] #to_do : train column add 
  
  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    return self.data.iloc[idx, 0 ] , torch.tensor(self.data.iloc[idx, 1:].to_numpy(),dtype=torch.float, device = device)

survive_test = SurviveDataset(test)
survive_testsetload = DataLoader(dataset=survive_test , batch_size=batch_size, shuffle=False)

next(iter(survive_testsetload))

test_result = []
pclass_ids = []
for idx, ( p_class , x) in enumerate(survive_testsetload):
  x = x.unsqueeze(2).float().to(device)
  result = encoder(x).squeeze(1).cpu().detach().numpy()
  test_result.append(result)
  pclass_ids.append(p_class)

test_result = np.concatenate(test_result, axis=0)
pclass_ids = np.concatenate(pclass_ids, axis=0)

total = len(test_result)
correct = 0
for inx, x in enumerate(test_result):
  if train.iloc[(np.abs(np.array(tmp_result) - x)).argmin()].Pclass.item() == pclass_result[inx]:
    correct +=1
print(correct/total)

np.abs(np.array(tmp_result) - x).argmin()

train.iloc[533]

test.iloc[inx]

#survive 예측
total = len(test_result)
prediction = []
for inx, x in enumerate(test_result):
  prediction.append(train.iloc[(np.abs(np.array(tmp_result) - x)).argmin()].Survived.item())

pclass_ids

submission = pd.DataFrame({
        "PassengerId": pclass_ids,
        "Survived": prediction
    })
submission.to_csv('submission.csv', index=False)

!kaggle competitions submit -c titanic -f submission.csv -m "Message"

