{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "타이타닉_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "YSyYWZagGXFR",
        "outputId": "ec3aee89-0dcb-4a3d-a3e5-f22709987807"
      },
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-464510ad-0798-4195-86c7-5c6f3e59d29b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-464510ad-0798-4195-86c7-5c6f3e59d29b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"jinkim89\",\"key\":\"097591ab6ed3746b82c87eb8ec6edb3e\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvgWtaS7Grjp",
        "outputId": "71b9b9aa-e764-4bfd-d465-965588369099"
      },
      "source": [
        "ls -1ha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2_8FJ7EHZ91",
        "outputId": "794b811d-5def-4ba2-de49-2287d3331bd6"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# Permission Warning 이 일어나지 않도록 \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# 본인이 참가한 모든 대회 보기 \n",
        "!kaggle competitions list"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "contradictory-my-dear-watson                   2030-07-01 23:59:00  Getting Started     Prizes        107           False  \n",
            "gan-getting-started                            2030-07-01 23:59:00  Getting Started     Prizes        212           False  \n",
            "tpu-getting-started                            2030-06-03 23:59:00  Getting Started  Knowledge        576           False  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       3528           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge      28555            True  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       7297           False  \n",
            "connectx                                       2030-01-01 00:00:00  Getting Started  Knowledge        625           False  \n",
            "nlp-getting-started                            2030-01-01 00:00:00  Getting Started  Knowledge       1902           False  \n",
            "competitive-data-science-predict-future-sales  2022-12-31 23:59:00  Playground           Kudos      10823           False  \n",
            "jane-street-market-prediction                  2021-08-23 23:59:00  Featured          $100,000       4245           False  \n",
            "hungry-geese                                   2021-07-26 23:59:00  Playground          Prizes        440           False  \n",
            "coleridgeinitiative-show-us-the-data           2021-06-22 23:59:00  Featured           $90,000        298           False  \n",
            "bms-molecular-translation                      2021-06-02 23:59:00  Featured           $50,000        391           False  \n",
            "birdclef-2021                                  2021-05-31 23:59:00  Research            $5,000         49           False  \n",
            "iwildcam2021-fgvc8                             2021-05-26 23:59:00  Research         Knowledge         14           False  \n",
            "herbarium-2021-fgvc8                           2021-05-26 23:59:00  Research         Knowledge         34           False  \n",
            "plant-pathology-2021-fgvc8                     2021-05-26 23:59:00  Research         Knowledge        159           False  \n",
            "hotel-id-2021-fgvc8                            2021-05-26 23:59:00  Research         Knowledge         34           False  \n",
            "hashcode-2021-oqr-extension                    2021-05-25 23:59:00  Playground       Knowledge        105           False  \n",
            "indoor-location-navigation                     2021-05-17 23:59:00  Research           $10,000        814           False  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GirHO6U2Heez",
        "outputId": "70003f20-b513-4cb5-8d8d-7685664aaf69"
      },
      "source": [
        "!kaggle competitions download -c titanic"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/59.8k [00:00<?, ?B/s]\n",
            "100% 59.8k/59.8k [00:00<00:00, 22.8MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/28.0k [00:00<?, ?B/s]\n",
            "100% 28.0k/28.0k [00:00<00:00, 26.7MB/s]\n",
            "Downloading gender_submission.csv to /content\n",
            "  0% 0.00/3.18k [00:00<?, ?B/s]\n",
            "100% 3.18k/3.18k [00:00<00:00, 2.15MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD-rSqMXH-7K",
        "outputId": "6a7453f6-747a-4199-8963-d0cbf8a0504d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gender_submission.csv  kaggle.json  sample_data  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czS4tRe6O8hf"
      },
      "source": [
        "batch_size = 32"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGLeQw6WIBwz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_YZUrK5NLjPp",
        "outputId": "978be2a4-e4e6-4364-b569-0da053c6abe7"
      },
      "source": [
        "test.describe()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>418.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>332.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>418.000000</td>\n",
              "      <td>417.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1100.500000</td>\n",
              "      <td>2.265550</td>\n",
              "      <td>30.272590</td>\n",
              "      <td>0.447368</td>\n",
              "      <td>0.392344</td>\n",
              "      <td>35.627188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>120.810458</td>\n",
              "      <td>0.841838</td>\n",
              "      <td>14.181209</td>\n",
              "      <td>0.896760</td>\n",
              "      <td>0.981429</td>\n",
              "      <td>55.907576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>892.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>996.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.895800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1100.500000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1204.750000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1309.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
              "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
              "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
              "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
              "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
              "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
              "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
              "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
              "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "nsyUbZ5ELk8a",
        "outputId": "fe0fcf4b-f0a6-4758-c677-6cd787f10e18"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>714.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.383838</td>\n",
              "      <td>2.308642</td>\n",
              "      <td>29.699118</td>\n",
              "      <td>0.523008</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>32.204208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>257.353842</td>\n",
              "      <td>0.486592</td>\n",
              "      <td>0.836071</td>\n",
              "      <td>14.526497</td>\n",
              "      <td>1.102743</td>\n",
              "      <td>0.806057</td>\n",
              "      <td>49.693429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>223.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>668.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n",
              "count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n",
              "mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n",
              "std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n",
              "min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n",
              "25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n",
              "50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n",
              "75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n",
              "max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T0Q-vm09l1B"
      },
      "source": [
        "cat_dict={}\n",
        "def categorical_encoding(data):\n",
        "  col = data.name\n",
        "  if col not in cat_dict:\n",
        "    cat_dict[col] = {}\n",
        "  result = []\n",
        "  for x in data:\n",
        "    if x not in cat_dict[col] :\n",
        "      if len(cat_dict[col]) == 0:\n",
        "        label = 0    \n",
        "      else:\n",
        "        label = max(cat_dict[col].values())+1\n",
        "      cat_dict[col][x] = label\n",
        "    result.append(cat_dict[col][x])\n",
        "  return np.array(result)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C-Jb0lIMPy1"
      },
      "source": [
        "def preprocessing_data(dataset , onehot=False):\n",
        "  #name에서 title 추출\n",
        "  dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.')\n",
        "\n",
        "  #missing value 처리\n",
        "  #Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'] => 컬럼은 총 12개\n",
        "  #print(train.isnull().sum())\n",
        "  # Age , Cabin, Embarked 3개 컬럼 missing\n",
        "  # Missing value는 임의의 다른 값으로 fill 처리\n",
        "  dataset['Age'].fillna(0 , inplace=True) #Age 는 0으로\n",
        "  dataset['Cabin'].fillna('N00' , inplace=True) #Cabin 은 N00\n",
        "  dataset['Embarked'].fillna('N' , inplace=True) #Embarked는 N으로\n",
        "\n",
        "  #categorical data 처리\n",
        "  # Sex, Ticket, Cabin, Embarked, Title\n",
        "  cate_col = ['Sex' , 'Ticket' , 'Cabin' , 'Embarked' , 'Title']\n",
        "  \n",
        "  for col in cate_col :\n",
        "    # col_label = train[col].astype('category').cat.codes\n",
        "    col_label = categorical_encoding(dataset[col])\n",
        "    if onehot is True:\n",
        "      num = np.unique(col_label, axis=0).shape[0]\n",
        "      encoding = np.eye(num)[col_label]\n",
        "      dataset[col+'_encoding'] = encoding\n",
        "    else:\n",
        "      dataset[col+'_encoding'] = col_label\n",
        "\n",
        "  return dataset\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "O4zfr9t5MNH5",
        "outputId": "ae76e7e9-12a1-472b-89b7-0be5cf90b151"
      },
      "source": [
        "train = preprocessing_data(train)\n",
        "train.isnull().sum() #missing value 없음\n",
        "train.head()"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Title</th>\n",
              "      <th>Sex_encoding</th>\n",
              "      <th>Ticket_encoding</th>\n",
              "      <th>Cabin_encoding</th>\n",
              "      <th>Embarked_encoding</th>\n",
              "      <th>Title_encoding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>N00</td>\n",
              "      <td>S</td>\n",
              "      <td>Mr</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>N00</td>\n",
              "      <td>S</td>\n",
              "      <td>Miss</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>N00</td>\n",
              "      <td>S</td>\n",
              "      <td>Mr</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  ...  Embarked_encoding Title_encoding\n",
              "0            1         0  ...                  0              0\n",
              "1            2         1  ...                  1              1\n",
              "2            3         1  ...                  0              2\n",
              "3            4         1  ...                  0              1\n",
              "4            5         0  ...                  0              0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZh59O3MrxOl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c425d2af-11ea-4ad2-c5c8-b775e8eb2065"
      },
      "source": [
        "#실제 학습에 사용할 데이터는 sequential 데이터와 labeling 된 데이터\n",
        "trainset = train[['PassengerId' , 'Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding' , 'Embarked_encoding' , 'Title_encoding']]\n",
        "trainset"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_encoding</th>\n",
              "      <th>Ticket_encoding</th>\n",
              "      <th>Cabin_encoding</th>\n",
              "      <th>Embarked_encoding</th>\n",
              "      <th>Title_encoding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>2</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>677</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>678</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>1</td>\n",
              "      <td>614</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>679</td>\n",
              "      <td>147</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>3</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>0</td>\n",
              "      <td>680</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId  Pclass  ...  Embarked_encoding  Title_encoding\n",
              "0              1       3  ...                  0               0\n",
              "1              2       1  ...                  1               1\n",
              "2              3       3  ...                  0               2\n",
              "3              4       1  ...                  0               1\n",
              "4              5       3  ...                  0               0\n",
              "..           ...     ...  ...                ...             ...\n",
              "886          887       2  ...                  0               5\n",
              "887          888       1  ...                  0               2\n",
              "888          889       3  ...                  0               2\n",
              "889          890       1  ...                  1               0\n",
              "890          891       3  ...                  2               0\n",
              "\n",
              "[891 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnwiMJmKuV0N"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader  "
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Uco4X8PsC7"
      },
      "source": [
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "RoDdmW2-U0zh",
        "outputId": "4a83bd72-d883-4038-efd9-3ad14f43e6a9"
      },
      "source": [
        "test"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>1305</td>\n",
              "      <td>3</td>\n",
              "      <td>Spector, Mr. Woolf</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A.5. 3236</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>1306</td>\n",
              "      <td>1</td>\n",
              "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
              "      <td>female</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17758</td>\n",
              "      <td>108.9000</td>\n",
              "      <td>C105</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>1307</td>\n",
              "      <td>3</td>\n",
              "      <td>Saether, Mr. Simon Sivertsen</td>\n",
              "      <td>male</td>\n",
              "      <td>38.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SOTON/O.Q. 3101262</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>1308</td>\n",
              "      <td>3</td>\n",
              "      <td>Ware, Mr. Frederick</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>359309</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>1309</td>\n",
              "      <td>3</td>\n",
              "      <td>Peter, Master. Michael J</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2668</td>\n",
              "      <td>22.3583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>418 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId  Pclass  ... Cabin Embarked\n",
              "0            892       3  ...   NaN        Q\n",
              "1            893       3  ...   NaN        S\n",
              "2            894       2  ...   NaN        Q\n",
              "3            895       3  ...   NaN        S\n",
              "4            896       3  ...   NaN        S\n",
              "..           ...     ...  ...   ...      ...\n",
              "413         1305       3  ...   NaN        S\n",
              "414         1306       1  ...  C105        C\n",
              "415         1307       3  ...   NaN        S\n",
              "416         1308       3  ...   NaN        S\n",
              "417         1309       3  ...   NaN        C\n",
              "\n",
              "[418 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvHpsBgemJeb"
      },
      "source": [
        "trainset = train[['Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding' , 'Embarked_encoding' , 'Title_encoding']] #to_do : train column add\n",
        "train_dataloader = DataLoader(dataset=trainset.to_numpy() , batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qfjln4Z1zbq"
      },
      "source": [
        "#custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "  # def __init__(self, data, column):\n",
        "  def __init__(self, data):\n",
        "    p_data = preprocessing_data(data)\n",
        "    self.data = p_data[['Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding', 'Embarked_encoding' , 'Title_encoding']] #to_do : train column add \n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data.iloc[idx, 0 ] , torch.tensor(self.data.iloc[idx, 1:].to_numpy(),dtype=torch.float, device = device)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSis05BunNxn"
      },
      "source": [
        "test_dataset = CustomDataset(test)\n",
        "test_dataloader = DataLoader(dataset=test_dataset , batch_size = batch_size , shuffle=False)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDnd9o3ooPRF"
      },
      "source": [
        "def _same_pad(k=1, dil=1):\n",
        "    # assumes stride length of 1\n",
        "    # p = math.ceil((l - 1) * s - l + dil*(k - 1) + 1)\n",
        "    p = math.ceil(dil*(k - 1))\n",
        "    #print(\"padding:\", p)\n",
        "    return p\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "        Note To Self:  using padding to \"mask\" the convolution is equivalent to\n",
        "        either centering the convolution (no mask) or skewing the convolution to\n",
        "        the left (mask).  Either way, we should end up with n timesteps.\n",
        "        Also note that \"masked convolution\" and \"casual convolution\" are two\n",
        "        names for the same thing.\n",
        "    Args:\n",
        "        d (int): size of inner track of network.\n",
        "        r (int): size of dilation\n",
        "        k (int): size of kernel in dilated convolution (odd numbers only)\n",
        "        casual (bool): determines how to pad the casual conv layer. See notes.\n",
        "    \"\"\"\n",
        "    def __init__(self, d, r=1, k=3, casual=False, use_bias=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.d = d # input features\n",
        "        self.r = r # dilation size\n",
        "        self.k = k # \"masked kernel size\"\n",
        "        ub = use_bias\n",
        "        self.layernorm1 = nn.InstanceNorm1d(num_features=2*d, affine=True) # same as LayerNorm\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1x1_1 = nn.Conv1d(2*d, d, kernel_size=1, bias=ub) # output is \"d\"\n",
        "        self.layernorm2 = nn.InstanceNorm1d(num_features=d, affine=True)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        if casual:\n",
        "            padding = (_same_pad(k,r), 0)\n",
        "        else:\n",
        "            p = _same_pad(k,r)\n",
        "            if p % 2 == 1:\n",
        "                padding = [p // 2 + 1, p // 2]\n",
        "            else:\n",
        "                padding = (p // 2, p // 2)\n",
        "        self.pad = nn.ConstantPad1d(padding, 0.)\n",
        "        #self.pad = nn.ReflectionPad1d(padding) # this might be better for audio\n",
        "        self.maskedconv1xk = nn.Conv1d(d, d, kernel_size=k, dilation=r, bias=ub)\n",
        "        self.layernorm3 = nn.InstanceNorm1d(num_features=d, affine=True)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.conv1x1_2 = nn.Conv1d(d, 2*d, kernel_size=1, bias=ub) # output is \"2*d\"\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        x = self.layernorm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv1x1_1(x)\n",
        "\n",
        "        x = self.layernorm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pad(x)\n",
        "        x = self.maskedconv1xk(x)\n",
        "\n",
        "        x = self.layernorm3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.conv1x1_2(x)\n",
        "        #print(\"ResBlock:\", x.size(), input.size())\n",
        "        x += input # add back in residual\n",
        "        return x\n",
        "\n",
        "class ResBlockSet(nn.Module):\n",
        "    \"\"\"\n",
        "        The Bytenet encoder and decoder are made up of sets of residual blocks\n",
        "        with dilations of increasing size.  These sets are then stacked upon each\n",
        "        other to create the full network.\n",
        "    \"\"\"\n",
        "    def __init__(self, d, max_r=16, k=3, casual=False , mt=None):\n",
        "        super(ResBlockSet, self).__init__()\n",
        "        self.d = d\n",
        "        self.max_r = max_r\n",
        "        self.k = k\n",
        "        self.mt = mt\n",
        "        rlist = [1 << x for x in range(15) if (1 << x) <= max_r]\n",
        "        self.blocks = nn.Sequential(*[ResBlock(d, r, k, casual) for r in rlist])\n",
        "\n",
        "        # if self.mt =='encoder':\n",
        "        #   self.avg_pooling = nn.AvgPool1d(3, stride=2)\n",
        "        # elif self.mt =='decoder':\n",
        "        #   self.deconv = nn.ConvTranspose1d(d*2 , d*2 , 7, 6)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        \n",
        "        # if self.mt =='encoder':\n",
        "        #   x = self.avg_pooling(x)\n",
        "        \n",
        "        # elif self.mt =='decoder':\n",
        "        #   x = self.deconv(x)\n",
        "     \n",
        "        return x\n",
        "\n",
        "class BytenetEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "        d = hidden units\n",
        "        max_r = maximum dilation rate (paper default: 16)\n",
        "        k = masked kernel size (paper default: 3)\n",
        "        num_sets = number of residual sets (paper default: 6. 5x6 = 30 ResBlocks)\n",
        "        a = relative length of output sequence\n",
        "        b = output sequence length intercept\n",
        "    \"\"\"\n",
        "    def __init__(self, d=800, max_r=16, k=3, num_sets=6):\n",
        "        super(BytenetEncoder, self).__init__()\n",
        "        self.d = d\n",
        "        self.max_r = max_r\n",
        "        self.k = k\n",
        "        self.num_sets = num_sets\n",
        "        self.pad_in = nn.ConstantPad1d((0, 1), 0.)\n",
        "        self.conv_in = nn.Conv1d(d, 2*d , 1)\n",
        "        self.sets = nn.Sequential()\n",
        "        for i in range(num_sets):\n",
        "            self.sets.add_module(\"set_{}\".format(i+1), ResBlockSet(d, max_r, k, mt=\"encoder\"))\n",
        "        # self.conv_out = nn.Conv1d(2*d, 2*d, 1)\n",
        "        self.conv_out = nn.Conv1d(2*d , 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        \n",
        "        x_len = x.size(-1)\n",
        "        x = self.conv_in(x)\n",
        "        x = self.sets(x)\n",
        "        x = self.conv_out(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "class SimpleEmbEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "        The decoder only training does not specify how to turn an integer input\n",
        "        into a 2*d dimensional vector that the decoder network expects, so I\n",
        "        decided to use a simple embedding network.\n",
        "    \"\"\"\n",
        "    def __init__(self, ne, ed):\n",
        "        super(SimpleEmbEncoder, self).__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings=ne, embedding_dim=ed)\n",
        "        self.ne = ne\n",
        "    def forward(self, input):\n",
        "        x = torch.clamp(input, 0, self.ne)\n",
        "        x = self.emb(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class BytenetDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "        d = hidden units\n",
        "        max_r = maximum dilation rate (paper default: 16)\n",
        "        k = masked kernel size (paper default: 3)\n",
        "        num_sets = number of residual sets (paper default: 6. 5x6 = 30 ResBlocks)\n",
        "        num_classes = number of output classes (Hunter prize default: 205)\n",
        "    \"\"\"\n",
        "    def __init__(self, d=512, max_r=16, k=3, num_sets=6, num_classes=205,\n",
        "                 reduce_out=None, use_logsm=True):\n",
        "        super(BytenetDecoder, self).__init__()\n",
        "        self.max_r = max_r\n",
        "        self.k = k\n",
        "        self.d = d\n",
        "        self.num_sets = num_sets\n",
        "        self.use_logsm = use_logsm # this is for NLLLoss\n",
        "        self.conv_in = nn.Conv1d(1,2*d,1)\n",
        "        self.sets = nn.Sequential()\n",
        "        for i in range(num_sets):\n",
        "            self.sets.add_module(\"set_{}\".format(i+1), ResBlockSet(d, max_r, k, True, mt=\"decoder\"))\n",
        "            if reduce_out is not None:\n",
        "                r = reduce_out[i]\n",
        "                if r != 0:\n",
        "                    reduce_conv = nn.Conv1d(2*d, 2*d, r, r)\n",
        "                    reduce_pad = nn.ConstantPad1d((0, r), 0.)\n",
        "                    self.sets.add_module(\"reduce_pad_{}\".format(i+1), reduce_pad)\n",
        "                    self.sets.add_module(\"reduce_{}\".format(i+1), reduce_conv)\n",
        "        self.conv1 = nn.Conv1d(2*d, 2*d, 1)\n",
        "        self.conv2 = nn.Conv1d(2*d, num_classes, 1)\n",
        "        #self.logsm = nn.LogSoftmax(dim=1)\n",
        "        self.fc = nn.Linear(2851, 3401)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        x = self.conv_in(x)\n",
        "        x = self.sets(x)\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        # if self.use_logsm:\n",
        "            # x = self.logsm(x)\n",
        "        # print(x.shape)\n",
        "        # x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBZP0sqHslSR",
        "cellView": "code"
      },
      "source": [
        "#@title 기본 제목 텍스트\n",
        "#hyper para 설정\n",
        "learning_rate = 0.00001\n",
        "input_features = trainset.columns.shape[0] -1\n",
        "max_r = 16\n",
        "k = 3\n",
        "num_sets = 6\n",
        "num_classes = 205\n",
        "epochs = 1\n",
        "max_length=3401\n",
        "feature_num = trainset.columns.shape[0] #9"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d348WeFCcRc",
        "outputId": "bbef4779-f39f-492e-fa63-eb15c283ced1"
      },
      "source": [
        "next(iter(train_dataloader)).shape"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1TokNlDq8GT"
      },
      "source": [
        "encoder = BytenetEncoder(feature_num, max_r, k, num_sets=4).to(device)\n",
        "decoder = BytenetDecoder(feature_num , max_r, k, num_sets=4, num_classes=feature_num, use_logsm=False).to(device)\n",
        "params = [{\"params\": encoder.parameters()}, {\"params\": decoder.parameters()}]\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# criterion = torch.nn.Smooth1Loss()\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(params)\n"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHJwJXHQLXDn"
      },
      "source": [
        "dataset_sizes = len(train)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSeTU52C0IG_"
      },
      "source": [
        "def train_model(encoder, decoder , criterion, optimizer , scheduler = None , num_epochs=25):\n",
        "  train_loss_history = []\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    avg_loss = 0.0\n",
        "\n",
        "    for idx , x in enumerate(train_dataloader):\n",
        "\n",
        "      encoder.zero_grad()\n",
        "      decoder.zero_grad()\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      x = x.unsqueeze(2).float().to(device)\n",
        "      output = encoder(x)\n",
        "      output = decoder(output)\n",
        "\n",
        "      loss = criterion(output, x)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      avg_loss += loss.item()\n",
        "\n",
        "      print(\"Epoch {}..... Step: {}/{}........ Average Loss {}\".format(epoch,(idx+1)*batch_size , dataset_sizes, avg_loss/(idx+1) ))\n",
        "\n",
        "      train_loss_history.append(avg_loss/(idx+1)*1000)\n",
        "  return encoder, decoder, train_loss_history "
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp6YVaqJ1ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1d198ef-5617-4b96-ddd8-79923c33a1b5"
      },
      "source": [
        "_, _ , train_loss_history = train_model(encoder, decoder, criterion, optimizer,None, 5000)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0..... Step: 32/891........ Average Loss 7.098460674285889\n",
            "Epoch 0..... Step: 64/891........ Average Loss 8.263861894607544\n",
            "Epoch 0..... Step: 96/891........ Average Loss 9.463005542755127\n",
            "Epoch 0..... Step: 128/891........ Average Loss 10.714524865150452\n",
            "Epoch 0..... Step: 160/891........ Average Loss 11.91107110977173\n",
            "Epoch 0..... Step: 192/891........ Average Loss 12.86048165957133\n",
            "Epoch 0..... Step: 224/891........ Average Loss 14.078989369528633\n",
            "Epoch 0..... Step: 256/891........ Average Loss 15.210231125354767\n",
            "Epoch 0..... Step: 288/891........ Average Loss 16.66933955086602\n",
            "Epoch 0..... Step: 320/891........ Average Loss 18.30506100654602\n",
            "Epoch 0..... Step: 352/891........ Average Loss 19.429809093475342\n",
            "Epoch 0..... Step: 384/891........ Average Loss 20.351570804913838\n",
            "Epoch 0..... Step: 416/891........ Average Loss 21.198547986837532\n",
            "Epoch 0..... Step: 448/891........ Average Loss 22.240358659199305\n",
            "Epoch 0..... Step: 480/891........ Average Loss 23.319895458221435\n",
            "Epoch 0..... Step: 512/891........ Average Loss 24.224845439195633\n",
            "Epoch 0..... Step: 544/891........ Average Loss 25.214375916649313\n",
            "Epoch 0..... Step: 576/891........ Average Loss 26.20534857114156\n",
            "Epoch 0..... Step: 608/891........ Average Loss 27.1797194480896\n",
            "Epoch 0..... Step: 640/891........ Average Loss 27.814549469947814\n",
            "Epoch 0..... Step: 672/891........ Average Loss 28.480184668586368\n",
            "Epoch 0..... Step: 704/891........ Average Loss 29.2781127799641\n",
            "Epoch 0..... Step: 736/891........ Average Loss 30.193215598230775\n",
            "Epoch 0..... Step: 768/891........ Average Loss 31.001838227113087\n",
            "Epoch 0..... Step: 800/891........ Average Loss 31.6634063911438\n",
            "Epoch 0..... Step: 832/891........ Average Loss 32.249559714243965\n",
            "Epoch 0..... Step: 864/891........ Average Loss 32.81405839213618\n",
            "Epoch 0..... Step: 896/891........ Average Loss 33.402262704713\n",
            "Epoch 1..... Step: 32/891........ Average Loss 6.059560298919678\n",
            "Epoch 1..... Step: 64/891........ Average Loss 6.368031740188599\n",
            "Epoch 1..... Step: 96/891........ Average Loss 7.113448937733968\n",
            "Epoch 1..... Step: 128/891........ Average Loss 7.915645956993103\n",
            "Epoch 1..... Step: 160/891........ Average Loss 8.698660373687744\n",
            "Epoch 1..... Step: 192/891........ Average Loss 9.295936504999796\n",
            "Epoch 1..... Step: 224/891........ Average Loss 10.143335546766009\n",
            "Epoch 1..... Step: 256/891........ Average Loss 10.847757279872894\n",
            "Epoch 1..... Step: 288/891........ Average Loss 11.798372003767225\n",
            "Epoch 1..... Step: 320/891........ Average Loss 12.841110563278198\n",
            "Epoch 1..... Step: 352/891........ Average Loss 13.530663013458252\n",
            "Epoch 1..... Step: 384/891........ Average Loss 14.074628949165344\n",
            "Epoch 1..... Step: 416/891........ Average Loss 14.583888677450327\n",
            "Epoch 1..... Step: 448/891........ Average Loss 15.239777394703456\n",
            "Epoch 1..... Step: 480/891........ Average Loss 15.8660875638326\n",
            "Epoch 1..... Step: 512/891........ Average Loss 16.305767387151718\n",
            "Epoch 1..... Step: 544/891........ Average Loss 16.79011656256283\n",
            "Epoch 1..... Step: 576/891........ Average Loss 17.26995062828064\n",
            "Epoch 1..... Step: 608/891........ Average Loss 17.636092411844356\n",
            "Epoch 1..... Step: 640/891........ Average Loss 17.82025201320648\n",
            "Epoch 1..... Step: 672/891........ Average Loss 18.042415232885453\n",
            "Epoch 1..... Step: 704/891........ Average Loss 18.36066443269903\n",
            "Epoch 1..... Step: 736/891........ Average Loss 18.59523572092471\n",
            "Epoch 1..... Step: 768/891........ Average Loss 18.83481429020564\n",
            "Epoch 1..... Step: 800/891........ Average Loss 18.887494106292724\n",
            "Epoch 1..... Step: 832/891........ Average Loss 18.855706710081833\n",
            "Epoch 1..... Step: 864/891........ Average Loss 18.7500699184559\n",
            "Epoch 1..... Step: 896/891........ Average Loss 18.61662236281804\n",
            "Epoch 2..... Step: 32/891........ Average Loss 9.447929382324219\n",
            "Epoch 2..... Step: 64/891........ Average Loss 8.889142990112305\n",
            "Epoch 2..... Step: 96/891........ Average Loss 8.64328384399414\n",
            "Epoch 2..... Step: 128/891........ Average Loss 8.998027801513672\n",
            "Epoch 2..... Step: 160/891........ Average Loss 9.042173957824707\n",
            "Epoch 2..... Step: 192/891........ Average Loss 9.167821089426676\n",
            "Epoch 2..... Step: 224/891........ Average Loss 9.352543694632393\n",
            "Epoch 2..... Step: 256/891........ Average Loss 9.46784234046936\n",
            "Epoch 2..... Step: 288/891........ Average Loss 10.122008641560873\n",
            "Epoch 2..... Step: 320/891........ Average Loss 10.856606483459473\n",
            "Epoch 2..... Step: 352/891........ Average Loss 11.191041166132147\n",
            "Epoch 2..... Step: 384/891........ Average Loss 11.224704821904501\n",
            "Epoch 2..... Step: 416/891........ Average Loss 10.984974274268517\n",
            "Epoch 2..... Step: 448/891........ Average Loss 10.838414941515241\n",
            "Epoch 2..... Step: 480/891........ Average Loss 10.72624937693278\n",
            "Epoch 2..... Step: 512/891........ Average Loss 10.657152354717255\n",
            "Epoch 2..... Step: 544/891........ Average Loss 10.596701004925896\n",
            "Epoch 2..... Step: 576/891........ Average Loss 10.550701565212673\n",
            "Epoch 2..... Step: 608/891........ Average Loss 10.543145681682386\n",
            "Epoch 2..... Step: 640/891........ Average Loss 10.507969999313355\n",
            "Epoch 2..... Step: 672/891........ Average Loss 10.56493718283517\n",
            "Epoch 2..... Step: 704/891........ Average Loss 10.785080866380172\n",
            "Epoch 2..... Step: 736/891........ Average Loss 10.830979222836701\n",
            "Epoch 2..... Step: 768/891........ Average Loss 11.054474194844564\n",
            "Epoch 2..... Step: 800/891........ Average Loss 11.099096260070802\n",
            "Epoch 2..... Step: 832/891........ Average Loss 11.110037180093618\n",
            "Epoch 2..... Step: 864/891........ Average Loss 11.102650183218497\n",
            "Epoch 2..... Step: 896/891........ Average Loss 11.0601555619921\n",
            "Epoch 3..... Step: 32/891........ Average Loss 9.026176452636719\n",
            "Epoch 3..... Step: 64/891........ Average Loss 8.498828887939453\n",
            "Epoch 3..... Step: 96/891........ Average Loss 8.246681531270346\n",
            "Epoch 3..... Step: 128/891........ Average Loss 8.511632919311523\n",
            "Epoch 3..... Step: 160/891........ Average Loss 8.441370391845703\n",
            "Epoch 3..... Step: 192/891........ Average Loss 8.410113970438639\n",
            "Epoch 3..... Step: 224/891........ Average Loss 8.429454530988421\n",
            "Epoch 3..... Step: 256/891........ Average Loss 8.389920115470886\n",
            "Epoch 3..... Step: 288/891........ Average Loss 8.803136825561523\n",
            "Epoch 3..... Step: 320/891........ Average Loss 9.286150455474854\n",
            "Epoch 3..... Step: 352/891........ Average Loss 9.456717231056906\n",
            "Epoch 3..... Step: 384/891........ Average Loss 9.383147478103638\n",
            "Epoch 3..... Step: 416/891........ Average Loss 9.106294008401724\n",
            "Epoch 3..... Step: 448/891........ Average Loss 8.931493316377912\n",
            "Epoch 3..... Step: 480/891........ Average Loss 8.86123317082723\n",
            "Epoch 3..... Step: 512/891........ Average Loss 8.867926865816116\n",
            "Epoch 3..... Step: 544/891........ Average Loss 8.868003424476175\n",
            "Epoch 3..... Step: 576/891........ Average Loss 8.880503469043308\n",
            "Epoch 3..... Step: 608/891........ Average Loss 8.911449357082969\n",
            "Epoch 3..... Step: 640/891........ Average Loss 8.907050967216492\n",
            "Epoch 3..... Step: 672/891........ Average Loss 8.971012410663423\n",
            "Epoch 3..... Step: 704/891........ Average Loss 9.173480835827915\n",
            "Epoch 3..... Step: 736/891........ Average Loss 9.21820937032285\n",
            "Epoch 3..... Step: 768/891........ Average Loss 9.407777488231659\n",
            "Epoch 3..... Step: 800/891........ Average Loss 9.424419384002686\n",
            "Epoch 3..... Step: 832/891........ Average Loss 9.400461141879742\n",
            "Epoch 3..... Step: 864/891........ Average Loss 9.40254831314087\n",
            "Epoch 3..... Step: 896/891........ Average Loss 9.386727009500776\n",
            "Epoch 4..... Step: 32/891........ Average Loss 8.21384048461914\n",
            "Epoch 4..... Step: 64/891........ Average Loss 7.740520238876343\n",
            "Epoch 4..... Step: 96/891........ Average Loss 7.49316946665446\n",
            "Epoch 4..... Step: 128/891........ Average Loss 7.688418865203857\n",
            "Epoch 4..... Step: 160/891........ Average Loss 7.58663387298584\n",
            "Epoch 4..... Step: 192/891........ Average Loss 7.514726479848226\n",
            "Epoch 4..... Step: 224/891........ Average Loss 7.47248785836356\n",
            "Epoch 4..... Step: 256/891........ Average Loss 7.393093407154083\n",
            "Epoch 4..... Step: 288/891........ Average Loss 7.711687882741292\n",
            "Epoch 4..... Step: 320/891........ Average Loss 8.084898900985717\n",
            "Epoch 4..... Step: 352/891........ Average Loss 8.212496367367832\n",
            "Epoch 4..... Step: 384/891........ Average Loss 8.151093006134033\n",
            "Epoch 4..... Step: 416/891........ Average Loss 7.931678185096154\n",
            "Epoch 4..... Step: 448/891........ Average Loss 7.822066988263812\n",
            "Epoch 4..... Step: 480/891........ Average Loss 7.795901044209798\n",
            "Epoch 4..... Step: 512/891........ Average Loss 7.8155593276023865\n",
            "Epoch 4..... Step: 544/891........ Average Loss 7.847522679497214\n",
            "Epoch 4..... Step: 576/891........ Average Loss 7.8547291490766735\n",
            "Epoch 4..... Step: 608/891........ Average Loss 7.870178599106638\n",
            "Epoch 4..... Step: 640/891........ Average Loss 7.853898596763611\n",
            "Epoch 4..... Step: 672/891........ Average Loss 7.8903064500717885\n",
            "Epoch 4..... Step: 704/891........ Average Loss 8.048056147315286\n",
            "Epoch 4..... Step: 736/891........ Average Loss 8.056408405303955\n",
            "Epoch 4..... Step: 768/891........ Average Loss 8.209454079469046\n",
            "Epoch 4..... Step: 800/891........ Average Loss 8.224416179656982\n",
            "Epoch 4..... Step: 832/891........ Average Loss 8.223156910676222\n",
            "Epoch 4..... Step: 864/891........ Average Loss 8.225429517251474\n",
            "Epoch 4..... Step: 896/891........ Average Loss 8.198338559695653\n",
            "Epoch 5..... Step: 32/891........ Average Loss 7.333249092102051\n",
            "Epoch 5..... Step: 64/891........ Average Loss 6.851317882537842\n",
            "Epoch 5..... Step: 96/891........ Average Loss 6.571783701578776\n",
            "Epoch 5..... Step: 128/891........ Average Loss 6.691334009170532\n",
            "Epoch 5..... Step: 160/891........ Average Loss 6.53184928894043\n",
            "Epoch 5..... Step: 192/891........ Average Loss 6.445963939030965\n",
            "Epoch 5..... Step: 224/891........ Average Loss 6.348802430289132\n",
            "Epoch 5..... Step: 256/891........ Average Loss 6.236624538898468\n",
            "Epoch 5..... Step: 288/891........ Average Loss 6.529163625505236\n",
            "Epoch 5..... Step: 320/891........ Average Loss 6.980304956436157\n",
            "Epoch 5..... Step: 352/891........ Average Loss 7.181649338115346\n",
            "Epoch 5..... Step: 384/891........ Average Loss 7.22601850827535\n",
            "Epoch 5..... Step: 416/891........ Average Loss 7.10019771869366\n",
            "Epoch 5..... Step: 448/891........ Average Loss 7.065332787377494\n",
            "Epoch 5..... Step: 480/891........ Average Loss 7.060851033528646\n",
            "Epoch 5..... Step: 512/891........ Average Loss 7.082767575979233\n",
            "Epoch 5..... Step: 544/891........ Average Loss 7.107667866875143\n",
            "Epoch 5..... Step: 576/891........ Average Loss 7.108680645624797\n",
            "Epoch 5..... Step: 608/891........ Average Loss 7.131895943691856\n",
            "Epoch 5..... Step: 640/891........ Average Loss 7.141318297386169\n",
            "Epoch 5..... Step: 672/891........ Average Loss 7.195787043798537\n",
            "Epoch 5..... Step: 704/891........ Average Loss 7.3807313442230225\n",
            "Epoch 5..... Step: 736/891........ Average Loss 7.412717466769011\n",
            "Epoch 5..... Step: 768/891........ Average Loss 7.567570388317108\n",
            "Epoch 5..... Step: 800/891........ Average Loss 7.593717861175537\n",
            "Epoch 5..... Step: 832/891........ Average Loss 7.597897951419537\n",
            "Epoch 5..... Step: 864/891........ Average Loss 7.616381627542001\n",
            "Epoch 5..... Step: 896/891........ Average Loss 7.61005573613303\n",
            "Epoch 6..... Step: 32/891........ Average Loss 6.992717742919922\n",
            "Epoch 6..... Step: 64/891........ Average Loss 6.5787670612335205\n",
            "Epoch 6..... Step: 96/891........ Average Loss 6.3740620613098145\n",
            "Epoch 6..... Step: 128/891........ Average Loss 6.547176718711853\n",
            "Epoch 6..... Step: 160/891........ Average Loss 6.468639278411866\n",
            "Epoch 6..... Step: 192/891........ Average Loss 6.425236543019612\n",
            "Epoch 6..... Step: 224/891........ Average Loss 6.4112484114510675\n",
            "Epoch 6..... Step: 256/891........ Average Loss 6.358635902404785\n",
            "Epoch 6..... Step: 288/891........ Average Loss 6.660931375291613\n",
            "Epoch 6..... Step: 320/891........ Average Loss 7.042544364929199\n",
            "Epoch 6..... Step: 352/891........ Average Loss 7.192314234646884\n",
            "Epoch 6..... Step: 384/891........ Average Loss 7.162379304567973\n",
            "Epoch 6..... Step: 416/891........ Average Loss 6.974749345045823\n",
            "Epoch 6..... Step: 448/891........ Average Loss 6.887049095971244\n",
            "Epoch 6..... Step: 480/891........ Average Loss 6.871920077006022\n",
            "Epoch 6..... Step: 512/891........ Average Loss 6.896136492490768\n",
            "Epoch 6..... Step: 544/891........ Average Loss 6.930081143098719\n",
            "Epoch 6..... Step: 576/891........ Average Loss 6.934688541624281\n",
            "Epoch 6..... Step: 608/891........ Average Loss 6.942765311190956\n",
            "Epoch 6..... Step: 640/891........ Average Loss 6.934321403503418\n",
            "Epoch 6..... Step: 672/891........ Average Loss 6.9768618402026945\n",
            "Epoch 6..... Step: 704/891........ Average Loss 7.158893346786499\n",
            "Epoch 6..... Step: 736/891........ Average Loss 7.188928666322128\n",
            "Epoch 6..... Step: 768/891........ Average Loss 7.344638884067535\n",
            "Epoch 6..... Step: 800/891........ Average Loss 7.366120853424072\n",
            "Epoch 6..... Step: 832/891........ Average Loss 7.36728389446552\n",
            "Epoch 6..... Step: 864/891........ Average Loss 7.383054415384929\n",
            "Epoch 6..... Step: 896/891........ Average Loss 7.364813038281032\n",
            "Epoch 7..... Step: 32/891........ Average Loss 6.722766399383545\n",
            "Epoch 7..... Step: 64/891........ Average Loss 6.299077987670898\n",
            "Epoch 7..... Step: 96/891........ Average Loss 6.103714466094971\n",
            "Epoch 7..... Step: 128/891........ Average Loss 6.274689078330994\n",
            "Epoch 7..... Step: 160/891........ Average Loss 6.196670055389404\n",
            "Epoch 7..... Step: 192/891........ Average Loss 6.160849332809448\n",
            "Epoch 7..... Step: 224/891........ Average Loss 6.144979272569929\n",
            "Epoch 7..... Step: 256/891........ Average Loss 6.097382009029388\n",
            "Epoch 7..... Step: 288/891........ Average Loss 6.400804148779975\n",
            "Epoch 7..... Step: 320/891........ Average Loss 6.792085790634156\n",
            "Epoch 7..... Step: 352/891........ Average Loss 6.952130967920477\n",
            "Epoch 7..... Step: 384/891........ Average Loss 6.937257210413615\n",
            "Epoch 7..... Step: 416/891........ Average Loss 6.76860757974478\n",
            "Epoch 7..... Step: 448/891........ Average Loss 6.698254346847534\n",
            "Epoch 7..... Step: 480/891........ Average Loss 6.694290033976237\n",
            "Epoch 7..... Step: 512/891........ Average Loss 6.722225368022919\n",
            "Epoch 7..... Step: 544/891........ Average Loss 6.749184243819293\n",
            "Epoch 7..... Step: 576/891........ Average Loss 6.745048575931126\n",
            "Epoch 7..... Step: 608/891........ Average Loss 6.762701787446675\n",
            "Epoch 7..... Step: 640/891........ Average Loss 6.761603760719299\n",
            "Epoch 7..... Step: 672/891........ Average Loss 6.801816758655367\n",
            "Epoch 7..... Step: 704/891........ Average Loss 6.968744624744762\n",
            "Epoch 7..... Step: 736/891........ Average Loss 6.998780727386475\n",
            "Epoch 7..... Step: 768/891........ Average Loss 7.16068019469579\n",
            "Epoch 7..... Step: 800/891........ Average Loss 7.187351188659668\n",
            "Epoch 7..... Step: 832/891........ Average Loss 7.185737995000986\n",
            "Epoch 7..... Step: 864/891........ Average Loss 7.200643080252188\n",
            "Epoch 7..... Step: 896/891........ Average Loss 7.1849932840892246\n",
            "Epoch 8..... Step: 32/891........ Average Loss 6.483611583709717\n",
            "Epoch 8..... Step: 64/891........ Average Loss 6.063966751098633\n",
            "Epoch 8..... Step: 96/891........ Average Loss 5.848999977111816\n",
            "Epoch 8..... Step: 128/891........ Average Loss 5.978625416755676\n",
            "Epoch 8..... Step: 160/891........ Average Loss 5.857761669158935\n",
            "Epoch 8..... Step: 192/891........ Average Loss 5.80856982866923\n",
            "Epoch 8..... Step: 224/891........ Average Loss 5.7616086687360495\n",
            "Epoch 8..... Step: 256/891........ Average Loss 5.685391902923584\n",
            "Epoch 8..... Step: 288/891........ Average Loss 5.999126964145237\n",
            "Epoch 8..... Step: 320/891........ Average Loss 6.437600231170654\n",
            "Epoch 8..... Step: 352/891........ Average Loss 6.616585644808683\n",
            "Epoch 8..... Step: 384/891........ Average Loss 6.621147394180298\n",
            "Epoch 8..... Step: 416/891........ Average Loss 6.452812121464656\n",
            "Epoch 8..... Step: 448/891........ Average Loss 6.375089543206351\n",
            "Epoch 8..... Step: 480/891........ Average Loss 6.366687488555908\n",
            "Epoch 8..... Step: 512/891........ Average Loss 6.404505044221878\n",
            "Epoch 8..... Step: 544/891........ Average Loss 6.452759574441349\n",
            "Epoch 8..... Step: 576/891........ Average Loss 6.4662173589070635\n",
            "Epoch 8..... Step: 608/891........ Average Loss 6.487253314570377\n",
            "Epoch 8..... Step: 640/891........ Average Loss 6.486141228675843\n",
            "Epoch 8..... Step: 672/891........ Average Loss 6.5422564915248325\n",
            "Epoch 8..... Step: 704/891........ Average Loss 6.725235592235219\n",
            "Epoch 8..... Step: 736/891........ Average Loss 6.765560875768247\n",
            "Epoch 8..... Step: 768/891........ Average Loss 6.923217157522838\n",
            "Epoch 8..... Step: 800/891........ Average Loss 6.958595848083496\n",
            "Epoch 8..... Step: 832/891........ Average Loss 6.971559341137226\n",
            "Epoch 8..... Step: 864/891........ Average Loss 6.99325344297621\n",
            "Epoch 8..... Step: 896/891........ Average Loss 6.992883035114834\n",
            "Epoch 9..... Step: 32/891........ Average Loss 6.29745626449585\n",
            "Epoch 9..... Step: 64/891........ Average Loss 5.890368938446045\n",
            "Epoch 9..... Step: 96/891........ Average Loss 5.658144315083821\n",
            "Epoch 9..... Step: 128/891........ Average Loss 5.764922857284546\n",
            "Epoch 9..... Step: 160/891........ Average Loss 5.62680721282959\n",
            "Epoch 9..... Step: 192/891........ Average Loss 5.576340754826863\n",
            "Epoch 9..... Step: 224/891........ Average Loss 5.539631434849331\n",
            "Epoch 9..... Step: 256/891........ Average Loss 5.499861359596252\n",
            "Epoch 9..... Step: 288/891........ Average Loss 5.8335574467976885\n",
            "Epoch 9..... Step: 320/891........ Average Loss 6.278543949127197\n",
            "Epoch 9..... Step: 352/891........ Average Loss 6.464722459966486\n",
            "Epoch 9..... Step: 384/891........ Average Loss 6.461783170700073\n",
            "Epoch 9..... Step: 416/891........ Average Loss 6.298609073345478\n",
            "Epoch 9..... Step: 448/891........ Average Loss 6.232425655637469\n",
            "Epoch 9..... Step: 480/891........ Average Loss 6.234086799621582\n",
            "Epoch 9..... Step: 512/891........ Average Loss 6.2723276019096375\n",
            "Epoch 9..... Step: 544/891........ Average Loss 6.31517682355993\n",
            "Epoch 9..... Step: 576/891........ Average Loss 6.3248727851443824\n",
            "Epoch 9..... Step: 608/891........ Average Loss 6.350667727620978\n",
            "Epoch 9..... Step: 640/891........ Average Loss 6.358474850654602\n",
            "Epoch 9..... Step: 672/891........ Average Loss 6.412520454043434\n",
            "Epoch 9..... Step: 704/891........ Average Loss 6.589996944774281\n",
            "Epoch 9..... Step: 736/891........ Average Loss 6.628353533537491\n",
            "Epoch 9..... Step: 768/891........ Average Loss 6.7884601354599\n",
            "Epoch 9..... Step: 800/891........ Average Loss 6.815836658477783\n",
            "Epoch 9..... Step: 832/891........ Average Loss 6.820249465795664\n",
            "Epoch 9..... Step: 864/891........ Average Loss 6.835733378375018\n",
            "Epoch 9..... Step: 896/891........ Average Loss 6.826062457902091\n",
            "Epoch 10..... Step: 32/891........ Average Loss 6.176658630371094\n",
            "Epoch 10..... Step: 64/891........ Average Loss 5.78862738609314\n",
            "Epoch 10..... Step: 96/891........ Average Loss 5.56790828704834\n",
            "Epoch 10..... Step: 128/891........ Average Loss 5.68117082118988\n",
            "Epoch 10..... Step: 160/891........ Average Loss 5.557177543640137\n",
            "Epoch 10..... Step: 192/891........ Average Loss 5.506250540415446\n",
            "Epoch 10..... Step: 224/891........ Average Loss 5.471315520150321\n",
            "Epoch 10..... Step: 256/891........ Average Loss 5.415863394737244\n",
            "Epoch 10..... Step: 288/891........ Average Loss 5.734229193793403\n",
            "Epoch 10..... Step: 320/891........ Average Loss 6.16734037399292\n",
            "Epoch 10..... Step: 352/891........ Average Loss 6.344495079734108\n",
            "Epoch 10..... Step: 384/891........ Average Loss 6.346187194188436\n",
            "Epoch 10..... Step: 416/891........ Average Loss 6.203269408299373\n",
            "Epoch 10..... Step: 448/891........ Average Loss 6.149236236299787\n",
            "Epoch 10..... Step: 480/891........ Average Loss 6.143953037261963\n",
            "Epoch 10..... Step: 512/891........ Average Loss 6.17394295334816\n",
            "Epoch 10..... Step: 544/891........ Average Loss 6.2304364933687095\n",
            "Epoch 10..... Step: 576/891........ Average Loss 6.253218942218357\n",
            "Epoch 10..... Step: 608/891........ Average Loss 6.274543461046721\n",
            "Epoch 10..... Step: 640/891........ Average Loss 6.2793233633041385\n",
            "Epoch 10..... Step: 672/891........ Average Loss 6.332439695085798\n",
            "Epoch 10..... Step: 704/891........ Average Loss 6.505570671775124\n",
            "Epoch 10..... Step: 736/891........ Average Loss 6.5501479895218555\n",
            "Epoch 10..... Step: 768/891........ Average Loss 6.7189523577690125\n",
            "Epoch 10..... Step: 800/891........ Average Loss 6.745666580200195\n",
            "Epoch 10..... Step: 832/891........ Average Loss 6.758060950499314\n",
            "Epoch 10..... Step: 864/891........ Average Loss 6.7878076058846935\n",
            "Epoch 10..... Step: 896/891........ Average Loss 6.7775751522609164\n",
            "Epoch 11..... Step: 32/891........ Average Loss 6.057866096496582\n",
            "Epoch 11..... Step: 64/891........ Average Loss 5.665664911270142\n",
            "Epoch 11..... Step: 96/891........ Average Loss 5.455070972442627\n",
            "Epoch 11..... Step: 128/891........ Average Loss 5.600675821304321\n",
            "Epoch 11..... Step: 160/891........ Average Loss 5.546478939056397\n",
            "Epoch 11..... Step: 192/891........ Average Loss 5.558079560597737\n",
            "Epoch 11..... Step: 224/891........ Average Loss 5.587096623011997\n",
            "Epoch 11..... Step: 256/891........ Average Loss 5.582793235778809\n",
            "Epoch 11..... Step: 288/891........ Average Loss 5.928703943888347\n",
            "Epoch 11..... Step: 320/891........ Average Loss 6.374376487731934\n",
            "Epoch 11..... Step: 352/891........ Average Loss 6.555687384171919\n",
            "Epoch 11..... Step: 384/891........ Average Loss 6.547632257143657\n",
            "Epoch 11..... Step: 416/891........ Average Loss 6.38285886324369\n",
            "Epoch 11..... Step: 448/891........ Average Loss 6.330732345581055\n",
            "Epoch 11..... Step: 480/891........ Average Loss 6.3441280682881676\n",
            "Epoch 11..... Step: 512/891........ Average Loss 6.387892156839371\n",
            "Epoch 11..... Step: 544/891........ Average Loss 6.430282929364373\n",
            "Epoch 11..... Step: 576/891........ Average Loss 6.437578572167291\n",
            "Epoch 11..... Step: 608/891........ Average Loss 6.4648033443250155\n",
            "Epoch 11..... Step: 640/891........ Average Loss 6.4748279571533205\n",
            "Epoch 11..... Step: 672/891........ Average Loss 6.522066184452602\n",
            "Epoch 11..... Step: 704/891........ Average Loss 6.698451063849709\n",
            "Epoch 11..... Step: 736/891........ Average Loss 6.740200540293818\n",
            "Epoch 11..... Step: 768/891........ Average Loss 6.904715935389201\n",
            "Epoch 11..... Step: 800/891........ Average Loss 6.942747097015381\n",
            "Epoch 11..... Step: 832/891........ Average Loss 6.952627915602464\n",
            "Epoch 11..... Step: 864/891........ Average Loss 6.979197925991482\n",
            "Epoch 11..... Step: 896/891........ Average Loss 6.976098486355373\n",
            "Epoch 12..... Step: 32/891........ Average Loss 6.0530781745910645\n",
            "Epoch 12..... Step: 64/891........ Average Loss 5.667448997497559\n",
            "Epoch 12..... Step: 96/891........ Average Loss 5.4497725168863935\n",
            "Epoch 12..... Step: 128/891........ Average Loss 5.563276648521423\n",
            "Epoch 12..... Step: 160/891........ Average Loss 5.474045467376709\n",
            "Epoch 12..... Step: 192/891........ Average Loss 5.467061440149943\n",
            "Epoch 12..... Step: 224/891........ Average Loss 5.467442171914237\n",
            "Epoch 12..... Step: 256/891........ Average Loss 5.4543696641922\n",
            "Epoch 12..... Step: 288/891........ Average Loss 5.799937036302355\n",
            "Epoch 12..... Step: 320/891........ Average Loss 6.244392776489258\n",
            "Epoch 12..... Step: 352/891........ Average Loss 6.424984585155141\n",
            "Epoch 12..... Step: 384/891........ Average Loss 6.41610062122345\n",
            "Epoch 12..... Step: 416/891........ Average Loss 6.2668561568626995\n",
            "Epoch 12..... Step: 448/891........ Average Loss 6.2085526670728415\n",
            "Epoch 12..... Step: 480/891........ Average Loss 6.214194933573405\n",
            "Epoch 12..... Step: 512/891........ Average Loss 6.248886436223984\n",
            "Epoch 12..... Step: 544/891........ Average Loss 6.297333436853745\n",
            "Epoch 12..... Step: 576/891........ Average Loss 6.316974507437812\n",
            "Epoch 12..... Step: 608/891........ Average Loss 6.339531948691921\n",
            "Epoch 12..... Step: 640/891........ Average Loss 6.334776449203491\n",
            "Epoch 12..... Step: 672/891........ Average Loss 6.387345268612816\n",
            "Epoch 12..... Step: 704/891........ Average Loss 6.573346398093483\n",
            "Epoch 12..... Step: 736/891........ Average Loss 6.613031739773958\n",
            "Epoch 12..... Step: 768/891........ Average Loss 6.7740833560625715\n",
            "Epoch 12..... Step: 800/891........ Average Loss 6.814895935058594\n",
            "Epoch 12..... Step: 832/891........ Average Loss 6.830687357829167\n",
            "Epoch 12..... Step: 864/891........ Average Loss 6.851702990355315\n",
            "Epoch 12..... Step: 896/891........ Average Loss 6.849236249923706\n",
            "Epoch 13..... Step: 32/891........ Average Loss 6.070279598236084\n",
            "Epoch 13..... Step: 64/891........ Average Loss 5.71741247177124\n",
            "Epoch 13..... Step: 96/891........ Average Loss 5.535706520080566\n",
            "Epoch 13..... Step: 128/891........ Average Loss 5.678019881248474\n",
            "Epoch 13..... Step: 160/891........ Average Loss 5.5925318717956545\n",
            "Epoch 13..... Step: 192/891........ Average Loss 5.56851053237915\n",
            "Epoch 13..... Step: 224/891........ Average Loss 5.552233287266323\n",
            "Epoch 13..... Step: 256/891........ Average Loss 5.513438642024994\n",
            "Epoch 13..... Step: 288/891........ Average Loss 5.839335494571262\n",
            "Epoch 13..... Step: 320/891........ Average Loss 6.289937925338745\n",
            "Epoch 13..... Step: 352/891........ Average Loss 6.492402987046675\n",
            "Epoch 13..... Step: 384/891........ Average Loss 6.499206980069478\n",
            "Epoch 13..... Step: 416/891........ Average Loss 6.333784653590276\n",
            "Epoch 13..... Step: 448/891........ Average Loss 6.251947777611869\n",
            "Epoch 13..... Step: 480/891........ Average Loss 6.245898087819417\n",
            "Epoch 13..... Step: 512/891........ Average Loss 6.293358206748962\n",
            "Epoch 13..... Step: 544/891........ Average Loss 6.354100115158978\n",
            "Epoch 13..... Step: 576/891........ Average Loss 6.374890989727444\n",
            "Epoch 13..... Step: 608/891........ Average Loss 6.3868097506071395\n",
            "Epoch 13..... Step: 640/891........ Average Loss 6.3811910390853885\n",
            "Epoch 13..... Step: 672/891........ Average Loss 6.425857407706125\n",
            "Epoch 13..... Step: 704/891........ Average Loss 6.600486755371094\n",
            "Epoch 13..... Step: 736/891........ Average Loss 6.6484396354011865\n",
            "Epoch 13..... Step: 768/891........ Average Loss 6.816081464290619\n",
            "Epoch 13..... Step: 800/891........ Average Loss 6.84158145904541\n",
            "Epoch 13..... Step: 832/891........ Average Loss 6.853968565280621\n",
            "Epoch 13..... Step: 864/891........ Average Loss 6.887687824390553\n",
            "Epoch 13..... Step: 896/891........ Average Loss 6.883778316634042\n",
            "Epoch 14..... Step: 32/891........ Average Loss 5.960061550140381\n",
            "Epoch 14..... Step: 64/891........ Average Loss 5.616125106811523\n",
            "Epoch 14..... Step: 96/891........ Average Loss 5.449760913848877\n",
            "Epoch 14..... Step: 128/891........ Average Loss 5.60797119140625\n",
            "Epoch 14..... Step: 160/891........ Average Loss 5.545716190338135\n",
            "Epoch 14..... Step: 192/891........ Average Loss 5.548023462295532\n",
            "Epoch 14..... Step: 224/891........ Average Loss 5.561434745788574\n",
            "Epoch 14..... Step: 256/891........ Average Loss 5.5454641580581665\n",
            "Epoch 14..... Step: 288/891........ Average Loss 5.869614495171441\n",
            "Epoch 14..... Step: 320/891........ Average Loss 6.283008861541748\n",
            "Epoch 14..... Step: 352/891........ Average Loss 6.454892418601296\n",
            "Epoch 14..... Step: 384/891........ Average Loss 6.4533103704452515\n",
            "Epoch 14..... Step: 416/891........ Average Loss 6.308074804452749\n",
            "Epoch 14..... Step: 448/891........ Average Loss 6.257233040673392\n",
            "Epoch 14..... Step: 480/891........ Average Loss 6.262892500559489\n",
            "Epoch 14..... Step: 512/891........ Average Loss 6.293000280857086\n",
            "Epoch 14..... Step: 544/891........ Average Loss 6.3349385261535645\n",
            "Epoch 14..... Step: 576/891........ Average Loss 6.3585528002844915\n",
            "Epoch 14..... Step: 608/891........ Average Loss 6.386672898342735\n",
            "Epoch 14..... Step: 640/891........ Average Loss 6.386210942268372\n",
            "Epoch 14..... Step: 672/891........ Average Loss 6.431129750751314\n",
            "Epoch 14..... Step: 704/891........ Average Loss 6.605110190131447\n",
            "Epoch 14..... Step: 736/891........ Average Loss 6.646146380383035\n",
            "Epoch 14..... Step: 768/891........ Average Loss 6.809643010298411\n",
            "Epoch 14..... Step: 800/891........ Average Loss 6.840056114196777\n",
            "Epoch 14..... Step: 832/891........ Average Loss 6.842946657767663\n",
            "Epoch 14..... Step: 864/891........ Average Loss 6.86937971468325\n",
            "Epoch 14..... Step: 896/891........ Average Loss 6.861296858106341\n",
            "Epoch 15..... Step: 32/891........ Average Loss 5.908759593963623\n",
            "Epoch 15..... Step: 64/891........ Average Loss 5.556312084197998\n",
            "Epoch 15..... Step: 96/891........ Average Loss 5.37299108505249\n",
            "Epoch 15..... Step: 128/891........ Average Loss 5.520471572875977\n",
            "Epoch 15..... Step: 160/891........ Average Loss 5.44119176864624\n",
            "Epoch 15..... Step: 192/891........ Average Loss 5.433013995488484\n",
            "Epoch 15..... Step: 224/891........ Average Loss 5.43872206551688\n",
            "Epoch 15..... Step: 256/891........ Average Loss 5.415696561336517\n",
            "Epoch 15..... Step: 288/891........ Average Loss 5.728015528784858\n",
            "Epoch 15..... Step: 320/891........ Average Loss 6.125173902511596\n",
            "Epoch 15..... Step: 352/891........ Average Loss 6.2926482720808545\n",
            "Epoch 15..... Step: 384/891........ Average Loss 6.327829837799072\n",
            "Epoch 15..... Step: 416/891........ Average Loss 6.200138568878174\n",
            "Epoch 15..... Step: 448/891........ Average Loss 6.12579219681876\n",
            "Epoch 15..... Step: 480/891........ Average Loss 6.146755313873291\n",
            "Epoch 15..... Step: 512/891........ Average Loss 6.208288997411728\n",
            "Epoch 15..... Step: 544/891........ Average Loss 6.269558008979349\n",
            "Epoch 15..... Step: 576/891........ Average Loss 6.285308811399672\n",
            "Epoch 15..... Step: 608/891........ Average Loss 6.318752263721667\n",
            "Epoch 15..... Step: 640/891........ Average Loss 6.332461023330689\n",
            "Epoch 15..... Step: 672/891........ Average Loss 6.38897568838937\n",
            "Epoch 15..... Step: 704/891........ Average Loss 6.571534590287642\n",
            "Epoch 15..... Step: 736/891........ Average Loss 6.620116192361583\n",
            "Epoch 15..... Step: 768/891........ Average Loss 6.789784987767537\n",
            "Epoch 15..... Step: 800/891........ Average Loss 6.820063228607178\n",
            "Epoch 15..... Step: 832/891........ Average Loss 6.815386221959041\n",
            "Epoch 15..... Step: 864/891........ Average Loss 6.827523884949861\n",
            "Epoch 15..... Step: 896/891........ Average Loss 6.8113084350313455\n",
            "Epoch 16..... Step: 32/891........ Average Loss 5.860342502593994\n",
            "Epoch 16..... Step: 64/891........ Average Loss 5.540547847747803\n",
            "Epoch 16..... Step: 96/891........ Average Loss 5.382594585418701\n",
            "Epoch 16..... Step: 128/891........ Average Loss 5.535560131072998\n",
            "Epoch 16..... Step: 160/891........ Average Loss 5.464114189147949\n",
            "Epoch 16..... Step: 192/891........ Average Loss 5.447460412979126\n",
            "Epoch 16..... Step: 224/891........ Average Loss 5.443590845380511\n",
            "Epoch 16..... Step: 256/891........ Average Loss 5.417442679405212\n",
            "Epoch 16..... Step: 288/891........ Average Loss 5.745417488945855\n",
            "Epoch 16..... Step: 320/891........ Average Loss 6.178646278381348\n",
            "Epoch 16..... Step: 352/891........ Average Loss 6.371342398903587\n",
            "Epoch 16..... Step: 384/891........ Average Loss 6.380119283994039\n",
            "Epoch 16..... Step: 416/891........ Average Loss 6.220041678502009\n",
            "Epoch 16..... Step: 448/891........ Average Loss 6.150649854115078\n",
            "Epoch 16..... Step: 480/891........ Average Loss 6.13532927831014\n",
            "Epoch 16..... Step: 512/891........ Average Loss 6.166511446237564\n",
            "Epoch 16..... Step: 544/891........ Average Loss 6.21076036902035\n",
            "Epoch 16..... Step: 576/891........ Average Loss 6.222164551417033\n",
            "Epoch 16..... Step: 608/891........ Average Loss 6.243206450813695\n",
            "Epoch 16..... Step: 640/891........ Average Loss 6.243104743957519\n",
            "Epoch 16..... Step: 672/891........ Average Loss 6.288306803930373\n",
            "Epoch 16..... Step: 704/891........ Average Loss 6.467779007824984\n",
            "Epoch 16..... Step: 736/891........ Average Loss 6.507803066917088\n",
            "Epoch 16..... Step: 768/891........ Average Loss 6.670334001382192\n",
            "Epoch 16..... Step: 800/891........ Average Loss 6.704308433532715\n",
            "Epoch 16..... Step: 832/891........ Average Loss 6.713858622771043\n",
            "Epoch 16..... Step: 864/891........ Average Loss 6.736425788314254\n",
            "Epoch 16..... Step: 896/891........ Average Loss 6.736538427216666\n",
            "Epoch 17..... Step: 32/891........ Average Loss 5.831841945648193\n",
            "Epoch 17..... Step: 64/891........ Average Loss 5.490524768829346\n",
            "Epoch 17..... Step: 96/891........ Average Loss 5.307939688364665\n",
            "Epoch 17..... Step: 128/891........ Average Loss 5.431607484817505\n",
            "Epoch 17..... Step: 160/891........ Average Loss 5.322565174102783\n",
            "Epoch 17..... Step: 192/891........ Average Loss 5.2866471608479815\n",
            "Epoch 17..... Step: 224/891........ Average Loss 5.263331821986607\n",
            "Epoch 17..... Step: 256/891........ Average Loss 5.209535539150238\n",
            "Epoch 17..... Step: 288/891........ Average Loss 5.525280687544081\n",
            "Epoch 17..... Step: 320/891........ Average Loss 5.942450094223022\n",
            "Epoch 17..... Step: 352/891........ Average Loss 6.127412709322843\n",
            "Epoch 17..... Step: 384/891........ Average Loss 6.147540807723999\n",
            "Epoch 17..... Step: 416/891........ Average Loss 6.000462715442364\n",
            "Epoch 17..... Step: 448/891........ Average Loss 5.933743306568691\n",
            "Epoch 17..... Step: 480/891........ Average Loss 5.9335590362548825\n",
            "Epoch 17..... Step: 512/891........ Average Loss 5.976751118898392\n",
            "Epoch 17..... Step: 544/891........ Average Loss 6.033697240492877\n",
            "Epoch 17..... Step: 576/891........ Average Loss 6.062279224395752\n",
            "Epoch 17..... Step: 608/891........ Average Loss 6.098254003022847\n",
            "Epoch 17..... Step: 640/891........ Average Loss 6.103836607933045\n",
            "Epoch 17..... Step: 672/891........ Average Loss 6.157181013198126\n",
            "Epoch 17..... Step: 704/891........ Average Loss 6.340431473471901\n",
            "Epoch 17..... Step: 736/891........ Average Loss 6.385853435682214\n",
            "Epoch 17..... Step: 768/891........ Average Loss 6.551659146944682\n",
            "Epoch 17..... Step: 800/891........ Average Loss 6.586586647033691\n",
            "Epoch 17..... Step: 832/891........ Average Loss 6.594070452910203\n",
            "Epoch 17..... Step: 864/891........ Average Loss 6.620677100287543\n",
            "Epoch 17..... Step: 896/891........ Average Loss 6.616903356143406\n",
            "Epoch 18..... Step: 32/891........ Average Loss 5.7982683181762695\n",
            "Epoch 18..... Step: 64/891........ Average Loss 5.483060121536255\n",
            "Epoch 18..... Step: 96/891........ Average Loss 5.322942733764648\n",
            "Epoch 18..... Step: 128/891........ Average Loss 5.472553610801697\n",
            "Epoch 18..... Step: 160/891........ Average Loss 5.392379951477051\n",
            "Epoch 18..... Step: 192/891........ Average Loss 5.37336277961731\n",
            "Epoch 18..... Step: 224/891........ Average Loss 5.368279933929443\n",
            "Epoch 18..... Step: 256/891........ Average Loss 5.325338661670685\n",
            "Epoch 18..... Step: 288/891........ Average Loss 5.637600104014079\n",
            "Epoch 18..... Step: 320/891........ Average Loss 6.054326105117798\n",
            "Epoch 18..... Step: 352/891........ Average Loss 6.2392870729619805\n",
            "Epoch 18..... Step: 384/891........ Average Loss 6.240575234095256\n",
            "Epoch 18..... Step: 416/891........ Average Loss 6.0798982840317946\n",
            "Epoch 18..... Step: 448/891........ Average Loss 6.014787810189383\n",
            "Epoch 18..... Step: 480/891........ Average Loss 6.010525798797607\n",
            "Epoch 18..... Step: 512/891........ Average Loss 6.0464344918727875\n",
            "Epoch 18..... Step: 544/891........ Average Loss 6.088966537924374\n",
            "Epoch 18..... Step: 576/891........ Average Loss 6.101248926586575\n",
            "Epoch 18..... Step: 608/891........ Average Loss 6.124149523283306\n",
            "Epoch 18..... Step: 640/891........ Average Loss 6.1283636569976805\n",
            "Epoch 18..... Step: 672/891........ Average Loss 6.180729616255987\n",
            "Epoch 18..... Step: 704/891........ Average Loss 6.354870557785034\n",
            "Epoch 18..... Step: 736/891........ Average Loss 6.39555083150449\n",
            "Epoch 18..... Step: 768/891........ Average Loss 6.555883228778839\n",
            "Epoch 18..... Step: 800/891........ Average Loss 6.587642040252685\n",
            "Epoch 18..... Step: 832/891........ Average Loss 6.597879538169274\n",
            "Epoch 18..... Step: 864/891........ Average Loss 6.62090379220468\n",
            "Epoch 18..... Step: 896/891........ Average Loss 6.620100242750985\n",
            "Epoch 19..... Step: 32/891........ Average Loss 5.758044242858887\n",
            "Epoch 19..... Step: 64/891........ Average Loss 5.437004566192627\n",
            "Epoch 19..... Step: 96/891........ Average Loss 5.261768658955892\n",
            "Epoch 19..... Step: 128/891........ Average Loss 5.39493989944458\n",
            "Epoch 19..... Step: 160/891........ Average Loss 5.2972533226013185\n",
            "Epoch 19..... Step: 192/891........ Average Loss 5.270264546076457\n",
            "Epoch 19..... Step: 224/891........ Average Loss 5.248119490487235\n",
            "Epoch 19..... Step: 256/891........ Average Loss 5.210874855518341\n",
            "Epoch 19..... Step: 288/891........ Average Loss 5.53718105951945\n",
            "Epoch 19..... Step: 320/891........ Average Loss 5.9569567203521725\n",
            "Epoch 19..... Step: 352/891........ Average Loss 6.1438963196494365\n",
            "Epoch 19..... Step: 384/891........ Average Loss 6.153452157974243\n",
            "Epoch 19..... Step: 416/891........ Average Loss 6.001930163456843\n",
            "Epoch 19..... Step: 448/891........ Average Loss 5.943212951932635\n",
            "Epoch 19..... Step: 480/891........ Average Loss 5.94858652750651\n",
            "Epoch 19..... Step: 512/891........ Average Loss 5.986140161752701\n",
            "Epoch 19..... Step: 544/891........ Average Loss 6.033867078668931\n",
            "Epoch 19..... Step: 576/891........ Average Loss 6.049552334679498\n",
            "Epoch 19..... Step: 608/891........ Average Loss 6.087099752928081\n",
            "Epoch 19..... Step: 640/891........ Average Loss 6.096311330795288\n",
            "Epoch 19..... Step: 672/891........ Average Loss 6.142948173341297\n",
            "Epoch 19..... Step: 704/891........ Average Loss 6.318340193141591\n",
            "Epoch 19..... Step: 736/891........ Average Loss 6.357821464538574\n",
            "Epoch 19..... Step: 768/891........ Average Loss 6.520704746246338\n",
            "Epoch 19..... Step: 800/891........ Average Loss 6.552589931488037\n",
            "Epoch 19..... Step: 832/891........ Average Loss 6.559108697451078\n",
            "Epoch 19..... Step: 864/891........ Average Loss 6.579776304739493\n",
            "Epoch 19..... Step: 896/891........ Average Loss 6.574572392872402\n",
            "Epoch 20..... Step: 32/891........ Average Loss 5.705803871154785\n",
            "Epoch 20..... Step: 64/891........ Average Loss 5.392592430114746\n",
            "Epoch 20..... Step: 96/891........ Average Loss 5.236604372660319\n",
            "Epoch 20..... Step: 128/891........ Average Loss 5.377873539924622\n",
            "Epoch 20..... Step: 160/891........ Average Loss 5.287430763244629\n",
            "Epoch 20..... Step: 192/891........ Average Loss 5.2603646119435625\n",
            "Epoch 20..... Step: 224/891........ Average Loss 5.240266050611224\n",
            "Epoch 20..... Step: 256/891........ Average Loss 5.2051355838775635\n",
            "Epoch 20..... Step: 288/891........ Average Loss 5.531176461113824\n",
            "Epoch 20..... Step: 320/891........ Average Loss 5.955082893371582\n",
            "Epoch 20..... Step: 352/891........ Average Loss 6.142612717368386\n",
            "Epoch 20..... Step: 384/891........ Average Loss 6.146538535753886\n",
            "Epoch 20..... Step: 416/891........ Average Loss 5.986820624424861\n",
            "Epoch 20..... Step: 448/891........ Average Loss 5.924235548291888\n",
            "Epoch 20..... Step: 480/891........ Average Loss 5.925182374318441\n",
            "Epoch 20..... Step: 512/891........ Average Loss 5.960297524929047\n",
            "Epoch 20..... Step: 544/891........ Average Loss 6.012265009038589\n",
            "Epoch 20..... Step: 576/891........ Average Loss 6.030283159679836\n",
            "Epoch 20..... Step: 608/891........ Average Loss 6.051126003265381\n",
            "Epoch 20..... Step: 640/891........ Average Loss 6.059391975402832\n",
            "Epoch 20..... Step: 672/891........ Average Loss 6.1195269993373325\n",
            "Epoch 20..... Step: 704/891........ Average Loss 6.301352457566694\n",
            "Epoch 20..... Step: 736/891........ Average Loss 6.3377765572589375\n",
            "Epoch 20..... Step: 768/891........ Average Loss 6.4997438589731855\n",
            "Epoch 20..... Step: 800/891........ Average Loss 6.530742263793945\n",
            "Epoch 20..... Step: 832/891........ Average Loss 6.537682184806237\n",
            "Epoch 20..... Step: 864/891........ Average Loss 6.563674714830187\n",
            "Epoch 20..... Step: 896/891........ Average Loss 6.559101326125009\n",
            "Epoch 21..... Step: 32/891........ Average Loss 5.6658711433410645\n",
            "Epoch 21..... Step: 64/891........ Average Loss 5.338945388793945\n",
            "Epoch 21..... Step: 96/891........ Average Loss 5.176537036895752\n",
            "Epoch 21..... Step: 128/891........ Average Loss 5.326321959495544\n",
            "Epoch 21..... Step: 160/891........ Average Loss 5.244108009338379\n",
            "Epoch 21..... Step: 192/891........ Average Loss 5.226726055145264\n",
            "Epoch 21..... Step: 224/891........ Average Loss 5.212568078722272\n",
            "Epoch 21..... Step: 256/891........ Average Loss 5.165024161338806\n",
            "Epoch 21..... Step: 288/891........ Average Loss 5.476635720994738\n",
            "Epoch 21..... Step: 320/891........ Average Loss 5.893739128112793\n",
            "Epoch 21..... Step: 352/891........ Average Loss 6.07617170160467\n",
            "Epoch 21..... Step: 384/891........ Average Loss 6.089100003242493\n",
            "Epoch 21..... Step: 416/891........ Average Loss 5.938665133256179\n",
            "Epoch 21..... Step: 448/891........ Average Loss 5.87997909954616\n",
            "Epoch 21..... Step: 480/891........ Average Loss 5.889256636301677\n",
            "Epoch 21..... Step: 512/891........ Average Loss 5.92999929189682\n",
            "Epoch 21..... Step: 544/891........ Average Loss 5.9768399911768295\n",
            "Epoch 21..... Step: 576/891........ Average Loss 6.003920714060466\n",
            "Epoch 21..... Step: 608/891........ Average Loss 6.028331053884406\n",
            "Epoch 21..... Step: 640/891........ Average Loss 6.039295268058777\n",
            "Epoch 21..... Step: 672/891........ Average Loss 6.10360354468936\n",
            "Epoch 21..... Step: 704/891........ Average Loss 6.297123258764094\n",
            "Epoch 21..... Step: 736/891........ Average Loss 6.352050615393597\n",
            "Epoch 21..... Step: 768/891........ Average Loss 6.520596146583557\n",
            "Epoch 21..... Step: 800/891........ Average Loss 6.566951007843017\n",
            "Epoch 21..... Step: 832/891........ Average Loss 6.583344202775222\n",
            "Epoch 21..... Step: 864/891........ Average Loss 6.606307859773989\n",
            "Epoch 21..... Step: 896/891........ Average Loss 6.602528878620693\n",
            "Epoch 22..... Step: 32/891........ Average Loss 5.616479396820068\n",
            "Epoch 22..... Step: 64/891........ Average Loss 5.303064346313477\n",
            "Epoch 22..... Step: 96/891........ Average Loss 5.156603813171387\n",
            "Epoch 22..... Step: 128/891........ Average Loss 5.3146045207977295\n",
            "Epoch 22..... Step: 160/891........ Average Loss 5.244239807128906\n",
            "Epoch 22..... Step: 192/891........ Average Loss 5.247309446334839\n",
            "Epoch 22..... Step: 224/891........ Average Loss 5.265189102717808\n",
            "Epoch 22..... Step: 256/891........ Average Loss 5.258625864982605\n",
            "Epoch 22..... Step: 288/891........ Average Loss 5.6098206837972\n",
            "Epoch 22..... Step: 320/891........ Average Loss 6.053760242462158\n",
            "Epoch 22..... Step: 352/891........ Average Loss 6.2525465705178\n",
            "Epoch 22..... Step: 384/891........ Average Loss 6.27151636282603\n",
            "Epoch 22..... Step: 416/891........ Average Loss 6.130983169262226\n",
            "Epoch 22..... Step: 448/891........ Average Loss 6.071213790348598\n",
            "Epoch 22..... Step: 480/891........ Average Loss 6.068306159973145\n",
            "Epoch 22..... Step: 512/891........ Average Loss 6.095404535531998\n",
            "Epoch 22..... Step: 544/891........ Average Loss 6.134778471554027\n",
            "Epoch 22..... Step: 576/891........ Average Loss 6.153022448221843\n",
            "Epoch 22..... Step: 608/891........ Average Loss 6.179306783174214\n",
            "Epoch 22..... Step: 640/891........ Average Loss 6.18227858543396\n",
            "Epoch 22..... Step: 672/891........ Average Loss 6.23335938226609\n",
            "Epoch 22..... Step: 704/891........ Average Loss 6.417938535863703\n",
            "Epoch 22..... Step: 736/891........ Average Loss 6.463976549065632\n",
            "Epoch 22..... Step: 768/891........ Average Loss 6.6405495206515\n",
            "Epoch 22..... Step: 800/891........ Average Loss 6.6849558067321775\n",
            "Epoch 22..... Step: 832/891........ Average Loss 6.697614339681772\n",
            "Epoch 22..... Step: 864/891........ Average Loss 6.721747945856165\n",
            "Epoch 22..... Step: 896/891........ Average Loss 6.708284275872367\n",
            "Epoch 23..... Step: 32/891........ Average Loss 5.564980983734131\n",
            "Epoch 23..... Step: 64/891........ Average Loss 5.272205591201782\n",
            "Epoch 23..... Step: 96/891........ Average Loss 5.147252241770427\n",
            "Epoch 23..... Step: 128/891........ Average Loss 5.34279191493988\n",
            "Epoch 23..... Step: 160/891........ Average Loss 5.310664558410645\n",
            "Epoch 23..... Step: 192/891........ Average Loss 5.318146785100301\n",
            "Epoch 23..... Step: 224/891........ Average Loss 5.347956453050886\n",
            "Epoch 23..... Step: 256/891........ Average Loss 5.342710614204407\n",
            "Epoch 23..... Step: 288/891........ Average Loss 5.6982421875\n",
            "Epoch 23..... Step: 320/891........ Average Loss 6.1647333145141605\n",
            "Epoch 23..... Step: 352/891........ Average Loss 6.381348263133656\n",
            "Epoch 23..... Step: 384/891........ Average Loss 6.395284215609233\n",
            "Epoch 23..... Step: 416/891........ Average Loss 6.235593575697679\n",
            "Epoch 23..... Step: 448/891........ Average Loss 6.173438310623169\n",
            "Epoch 23..... Step: 480/891........ Average Loss 6.159713935852051\n",
            "Epoch 23..... Step: 512/891........ Average Loss 6.186265736818314\n",
            "Epoch 23..... Step: 544/891........ Average Loss 6.227562287274529\n",
            "Epoch 23..... Step: 576/891........ Average Loss 6.232669565412733\n",
            "Epoch 23..... Step: 608/891........ Average Loss 6.253728715996993\n",
            "Epoch 23..... Step: 640/891........ Average Loss 6.253885221481323\n",
            "Epoch 23..... Step: 672/891........ Average Loss 6.300865105220249\n",
            "Epoch 23..... Step: 704/891........ Average Loss 6.4780050407756455\n",
            "Epoch 23..... Step: 736/891........ Average Loss 6.519790214041005\n",
            "Epoch 23..... Step: 768/891........ Average Loss 6.690649966398875\n",
            "Epoch 23..... Step: 800/891........ Average Loss 6.733315715789795\n",
            "Epoch 23..... Step: 832/891........ Average Loss 6.745685265614436\n",
            "Epoch 23..... Step: 864/891........ Average Loss 6.764511037755896\n",
            "Epoch 23..... Step: 896/891........ Average Loss 6.761380672454834\n",
            "Epoch 24..... Step: 32/891........ Average Loss 5.532424449920654\n",
            "Epoch 24..... Step: 64/891........ Average Loss 5.219217777252197\n",
            "Epoch 24..... Step: 96/891........ Average Loss 5.071612199147542\n",
            "Epoch 24..... Step: 128/891........ Average Loss 5.220762372016907\n",
            "Epoch 24..... Step: 160/891........ Average Loss 5.136595439910889\n",
            "Epoch 24..... Step: 192/891........ Average Loss 5.122341235478719\n",
            "Epoch 24..... Step: 224/891........ Average Loss 5.107129233224051\n",
            "Epoch 24..... Step: 256/891........ Average Loss 5.071219265460968\n",
            "Epoch 24..... Step: 288/891........ Average Loss 5.398424519432916\n",
            "Epoch 24..... Step: 320/891........ Average Loss 5.819560194015503\n",
            "Epoch 24..... Step: 352/891........ Average Loss 6.007145534862172\n",
            "Epoch 24..... Step: 384/891........ Average Loss 6.021692434946696\n",
            "Epoch 24..... Step: 416/891........ Average Loss 5.878957895132212\n",
            "Epoch 24..... Step: 448/891........ Average Loss 5.824666500091553\n",
            "Epoch 24..... Step: 480/891........ Average Loss 5.827635288238525\n",
            "Epoch 24..... Step: 512/891........ Average Loss 5.868867576122284\n",
            "Epoch 24..... Step: 544/891........ Average Loss 5.9225740713231705\n",
            "Epoch 24..... Step: 576/891........ Average Loss 5.939828634262085\n",
            "Epoch 24..... Step: 608/891........ Average Loss 5.974111632296913\n",
            "Epoch 24..... Step: 640/891........ Average Loss 5.982979011535645\n",
            "Epoch 24..... Step: 672/891........ Average Loss 6.036803472609747\n",
            "Epoch 24..... Step: 704/891........ Average Loss 6.223081198605624\n",
            "Epoch 24..... Step: 736/891........ Average Loss 6.276343096857485\n",
            "Epoch 24..... Step: 768/891........ Average Loss 6.445481499036153\n",
            "Epoch 24..... Step: 800/891........ Average Loss 6.487190990447998\n",
            "Epoch 24..... Step: 832/891........ Average Loss 6.5065385561722975\n",
            "Epoch 24..... Step: 864/891........ Average Loss 6.539029492272271\n",
            "Epoch 24..... Step: 896/891........ Average Loss 6.529078722000122\n",
            "Epoch 25..... Step: 32/891........ Average Loss 5.454602241516113\n",
            "Epoch 25..... Step: 64/891........ Average Loss 5.1629626750946045\n",
            "Epoch 25..... Step: 96/891........ Average Loss 5.011600176493327\n",
            "Epoch 25..... Step: 128/891........ Average Loss 5.176177024841309\n",
            "Epoch 25..... Step: 160/891........ Average Loss 5.125791454315186\n",
            "Epoch 25..... Step: 192/891........ Average Loss 5.1339772542317705\n",
            "Epoch 25..... Step: 224/891........ Average Loss 5.168571335928781\n",
            "Epoch 25..... Step: 256/891........ Average Loss 5.1659215092659\n",
            "Epoch 25..... Step: 288/891........ Average Loss 5.506595346662733\n",
            "Epoch 25..... Step: 320/891........ Average Loss 5.944754648208618\n",
            "Epoch 25..... Step: 352/891........ Average Loss 6.130803368308327\n",
            "Epoch 25..... Step: 384/891........ Average Loss 6.128519018491109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-223-84c89f7d4b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-222-e3a27f81fa29>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(encoder, decoder, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZMqnx_V6hLm",
        "outputId": "91ae0da7-e28b-490f-fde8-b901a22054bf"
      },
      "source": [
        "encoder(x).squeeze(1).cpu().detach().numpy().shape"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63oL5Ybe1miA"
      },
      "source": [
        "tmp_result = []\n",
        "passenger_id = []\n",
        "encoder.eval()\n",
        "for idx, x in enumerate(train_dataloader):\n",
        "  # print(x.shape)\n",
        "  x = x.unsqueeze(2).float().to(device)\n",
        "  new_x = encoder(x).squeeze(1).cpu().detach().numpy()\n",
        "  tmp_result.append(new_x)\n",
        "  # tmp_result.append(encoder(x.unsqueeze(-1).to(device)).squeeze(1).detach().cpu().numpy())\n",
        "  # passenger_id.append(id)\n",
        "tmp_result = np.concatenate(tmp_result, axis=0)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfbIIPxEIrNT"
      },
      "source": [
        "#custom dataset\n",
        "class SurviveDataset(Dataset):\n",
        "  # def __init__(self, data, column):\n",
        "  def __init__(self, data):\n",
        "    p_data = preprocessing_data(data)\n",
        "    self.data = p_data[['PassengerId' , 'Pclass' , 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'Sex_encoding' , 'Ticket_encoding' , 'Cabin_encoding', 'Embarked_encoding' , 'Title_encoding']] #to_do : train column add \n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data.iloc[idx, 0 ] , torch.tensor(self.data.iloc[idx, 1:].to_numpy(),dtype=torch.float, device = device)\n",
        "\n",
        "survive_test = SurviveDataset(test)\n",
        "survive_testsetload = DataLoader(dataset=survive_test , batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X2wmGvgJvHn",
        "outputId": "32c5f1e9-138b-41f1-9716-35900681f9c9"
      },
      "source": [
        "next(iter(survive_testsetload))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905,\n",
              "         906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919,\n",
              "         920, 921, 922, 923]),\n",
              " tensor([[  3.0000,  34.5000,   0.0000,   0.0000,   7.8292,   0.0000, 681.0000,\n",
              "            0.0000,   2.0000,   0.0000],\n",
              "         [  3.0000,  47.0000,   1.0000,   0.0000,   7.0000,   1.0000, 682.0000,\n",
              "            0.0000,   0.0000,   1.0000],\n",
              "         [  2.0000,  62.0000,   0.0000,   0.0000,   9.6875,   0.0000, 683.0000,\n",
              "            0.0000,   2.0000,   0.0000],\n",
              "         [  3.0000,  27.0000,   0.0000,   0.0000,   8.6625,   0.0000, 684.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  3.0000,  22.0000,   1.0000,   1.0000,  12.2875,   1.0000, 405.0000,\n",
              "            0.0000,   0.0000,   1.0000],\n",
              "         [  3.0000,  14.0000,   0.0000,   0.0000,   9.2250,   0.0000, 685.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  3.0000,  30.0000,   0.0000,   0.0000,   7.6292,   1.0000, 686.0000,\n",
              "            0.0000,   2.0000,   2.0000],\n",
              "         [  2.0000,  26.0000,   1.0000,   1.0000,  29.0000,   0.0000,  76.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  3.0000,  18.0000,   0.0000,   0.0000,   7.2292,   1.0000, 687.0000,\n",
              "            0.0000,   1.0000,   1.0000],\n",
              "         [  3.0000,  21.0000,   2.0000,   0.0000,  24.1500,   0.0000, 467.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  3.0000,   0.0000,   0.0000,   0.0000,   7.8958,   0.0000, 688.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  1.0000,  46.0000,   0.0000,   0.0000,  26.0000,   0.0000, 689.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  1.0000,  23.0000,   1.0000,   0.0000,  82.2667,   1.0000, 690.0000,\n",
              "          148.0000,   0.0000,   1.0000],\n",
              "         [  2.0000,  63.0000,   1.0000,   0.0000,  26.0000,   0.0000, 691.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  1.0000,  47.0000,   1.0000,   0.0000,  61.1750,   1.0000,  89.0000,\n",
              "           17.0000,   0.0000,   1.0000],\n",
              "         [  2.0000,  24.0000,   1.0000,   0.0000,  27.7208,   1.0000, 312.0000,\n",
              "            0.0000,   1.0000,   1.0000],\n",
              "         [  2.0000,  35.0000,   0.0000,   0.0000,  12.3500,   0.0000, 692.0000,\n",
              "            0.0000,   2.0000,   0.0000],\n",
              "         [  3.0000,  21.0000,   0.0000,   0.0000,   7.2250,   0.0000, 693.0000,\n",
              "            0.0000,   1.0000,   0.0000],\n",
              "         [  3.0000,  27.0000,   1.0000,   0.0000,   7.9250,   1.0000, 694.0000,\n",
              "            0.0000,   0.0000,   2.0000],\n",
              "         [  3.0000,  45.0000,   0.0000,   0.0000,   7.2250,   1.0000, 695.0000,\n",
              "            0.0000,   1.0000,   1.0000],\n",
              "         [  1.0000,  55.0000,   1.0000,   0.0000,  59.4000,   0.0000, 429.0000,\n",
              "            0.0000,   1.0000,   0.0000],\n",
              "         [  3.0000,   9.0000,   0.0000,   1.0000,   3.1708,   0.0000, 696.0000,\n",
              "            0.0000,   0.0000,   3.0000],\n",
              "         [  1.0000,   0.0000,   0.0000,   0.0000,  31.6833,   1.0000, 697.0000,\n",
              "            0.0000,   0.0000,   1.0000],\n",
              "         [  1.0000,  21.0000,   0.0000,   1.0000,  61.3792,   0.0000, 144.0000,\n",
              "            0.0000,   1.0000,   0.0000],\n",
              "         [  1.0000,  48.0000,   1.0000,   3.0000, 262.3750,   1.0000, 277.0000,\n",
              "           59.0000,   1.0000,   1.0000],\n",
              "         [  3.0000,  50.0000,   1.0000,   0.0000,  14.5000,   0.0000, 123.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  1.0000,  22.0000,   0.0000,   1.0000,  61.9792,   1.0000,  53.0000,\n",
              "          149.0000,   1.0000,   2.0000],\n",
              "         [  3.0000,  22.5000,   0.0000,   0.0000,   7.2250,   0.0000, 698.0000,\n",
              "            0.0000,   1.0000,   0.0000],\n",
              "         [  1.0000,  41.0000,   0.0000,   0.0000,  30.5000,   0.0000, 699.0000,\n",
              "          150.0000,   0.0000,   0.0000],\n",
              "         [  3.0000,   0.0000,   2.0000,   0.0000,  21.6792,   0.0000,  47.0000,\n",
              "            0.0000,   1.0000,   0.0000],\n",
              "         [  2.0000,  50.0000,   1.0000,   0.0000,  26.0000,   0.0000, 369.0000,\n",
              "            0.0000,   0.0000,   0.0000],\n",
              "         [  2.0000,  24.0000,   2.0000,   0.0000,  31.5000,   0.0000, 700.0000,\n",
              "            0.0000,   0.0000,   0.0000]], device='cuda:0')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JZlfzVE3Tuq"
      },
      "source": [
        "test_result = []\n",
        "pclass_ids = []\n",
        "for idx, ( p_class , x) in enumerate(survive_testsetload):\n",
        "  x = x.unsqueeze(2).float().to(device)\n",
        "  result = encoder(x).squeeze(1).cpu().detach().numpy()\n",
        "  test_result.append(result)\n",
        "  pclass_ids.append(p_class)\n",
        "\n",
        "test_result = np.concatenate(test_result, axis=0)\n",
        "pclass_ids = np.concatenate(pclass_ids, axis=0)"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCiufn-IpIf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056d07c8-f4fd-4802-9a58-2318d696189f"
      },
      "source": [
        "total = len(test_result)\n",
        "correct = 0\n",
        "for inx, x in enumerate(test_result):\n",
        "  if train.iloc[(np.abs(np.array(tmp_result) - x)).argmin()].Pclass.item() == pclass_result[inx]:\n",
        "    correct +=1\n",
        "print(correct/total)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.69377990430622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM0aCLOWPoP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5f228f-d67d-464e-fa71-f960c7a521f1"
      },
      "source": [
        "np.abs(np.array(tmp_result) - x).argmin()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "533"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghb2jwv088w5",
        "outputId": "8aaf7f53-3005-4c5a-e830-2915f8b7f8e6"
      },
      "source": [
        "train.iloc[533]"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId                                             534\n",
              "Survived                                                  1\n",
              "Pclass                                                    3\n",
              "Name                 Peter, Mrs. Catherine (Catherine Rizk)\n",
              "Sex                                                  female\n",
              "Age                                                       0\n",
              "SibSp                                                     0\n",
              "Parch                                                     2\n",
              "Ticket                                                 2668\n",
              "Fare                                                22.3583\n",
              "Cabin                                                   N00\n",
              "Embarked                                                  C\n",
              "Title                                                   Mrs\n",
              "Sex_encoding                                              1\n",
              "Ticket_encoding                                         119\n",
              "Cabin_encoding                                            0\n",
              "Embarked_encoding                                         1\n",
              "Title_encoding                                            1\n",
              "Name: 533, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJINGl6Q9Gh7",
        "outputId": "6ccdad65-6b71-4f7c-c76b-3fd39c98ef21"
      },
      "source": [
        "test.iloc[inx]"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId                              1309\n",
              "Pclass                                      3\n",
              "Name                 Peter, Master. Michael J\n",
              "Sex                                      male\n",
              "Age                                         0\n",
              "SibSp                                       1\n",
              "Parch                                       1\n",
              "Ticket                                   2668\n",
              "Fare                                  22.3583\n",
              "Cabin                                     N00\n",
              "Embarked                                    C\n",
              "Title                                  Master\n",
              "Sex_encoding                                0\n",
              "Ticket_encoding                           119\n",
              "Cabin_encoding                              0\n",
              "Embarked_encoding                           1\n",
              "Title_encoding                              3\n",
              "Name: 417, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiwphYwwFJlN"
      },
      "source": [
        "#survive 예측\n",
        "total = len(test_result)\n",
        "prediction = []\n",
        "for inx, x in enumerate(test_result):\n",
        "  prediction.append(train.iloc[(np.abs(np.array(tmp_result) - x)).argmin()].Survived.item())"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otQJyidgL5GI"
      },
      "source": [
        "pclass_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlaNpnH8LsgF"
      },
      "source": [
        "submission = pd.DataFrame({\n",
        "        \"PassengerId\": pclass_ids,\n",
        "        \"Survived\": prediction\n",
        "    })\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8ycCPMRL6by",
        "outputId": "bd1a7999-397d-4da8-9034-7788dab1d7e9"
      },
      "source": [
        "!kaggle competitions submit -c titanic -f submission.csv -m \"Message\""
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 2.77k/2.77k [00:02<00:00, 1.08kB/s]\n",
            "Successfully submitted to Titanic - Machine Learning from Disaster"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAxw-VC_MWl6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}